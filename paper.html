<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Fast Approximate Parallax Shadows</title>
</head>
<body>

<h1>Fast Approximate Parallax Shadows</h1>
<p><em>Principled Self-Shadowing for Height-Mapped Surfaces at Minimal Sample Counts</em></p>

<h2>Abstract</h2>

<p>We present Fast Approximate Parallax Shadows (FXPS), a set of improvements to Tatarchuk&rsquo;s [2006] soft POM shadow technique that target the low-iteration regime (N = 2&ndash;4) where linear sampling breaks down. Like the original, FXPS is an empirical heuristic designed for contexts where visual plausibility matters more than physical accuracy&mdash;stylized rendering, mobile GPUs, legacy pipelines, and performance-constrained material layers. It replaces the linear sampling distribution with a power-law function and the accumulation with a 1/<em>i</em> weighted maximum, producing a shadow probe whose effective sensitivity is <em>log-uniform</em> along the ray&mdash;equal attention is allocated to each multiplicative decade of distance. We show that this property is invariant across a single-parameter family of power-law exponents &alpha;, and that the correct shadow strength normalization is simply N (the iteration count), making sample count a pure quality knob.</p>

<p>We analyze the behavior of the power-law family and show that &alpha; = 0.5 (the square-root distribution) is the unique exponent whose distance-attenuation curve matches the solid-angle falloff of real occluders&mdash;the <em>t</em><sup>&minus;2</sup> relationship governing how much of the visible hemisphere an occluder blocks as a function of distance. At this exponent, samples concentrate in the far field where they complement rather than duplicate the normal map&rsquo;s existing near-field darkening, and the ray has risen sufficiently above the surface to avoid spurious self-occlusion from minor surface variations. The result is plausible distance-dependent shadow opacity and spatially soft shadow edges&mdash;arising structurally from the continuous depth-difference formulation&mdash;at 2&ndash;4 iterations per fragment, a meaningful visual improvement over Tatarchuk&rsquo;s linear soft shadows or no self-shadowing at all, for the same texture fetch budget.</p>

<h2>1. Introduction</h2>

<p>Tatarchuk [2006] introduced soft self-shadowing for Parallax Occlusion Mapping (POM) by replacing the binary intersection test of earlier techniques with a continuous depth-difference metric. This was a significant advance: rather than asking whether the height field intersects the light ray (producing hard, banded edges), the technique measures <em>by how much</em> each sample exceeds the ray, producing smooth shadow gradients that look plausible even at moderate sample counts. Her approach uses a linear distribution of samples along the ray, t(<em>i</em>) = <em>i</em>/<em>N</em>, with a distance-based attenuation factor to weight each sample&rsquo;s contribution.</p>

<p>At high iteration counts, this works well. At the very low sample counts demanded by mobile GPUs, legacy pipelines, and performance-constrained material layers (N = 2&ndash;4), however, the linear sampling strategy has two weaknesses. First, it distributes samples uniformly along the ray, wasting budget on regions that may be irrelevant to the local shadow topology. Second, it provides no natural mechanism for contact hardening&mdash;the perceptual property whereby shadows are darkest and sharpest near occluder-receiver contact, becoming lighter and wider with distance.</p>

<p>We present Fast Approximate Parallax Shadows (FXPS), a set of improvements to Tatarchuk&rsquo;s soft POM shadow technique that target the low-iteration regime. Like the original, FXPS is an empirical heuristic&mdash;not a physically based visibility computation&mdash;and is designed for contexts where visual plausibility matters more than physical accuracy. The improvements are:</p>

<p><strong>Power-law sampling with 1/<em>i</em> weighting.</strong> We replace the linear step distribution with a power-law function t(<em>i</em>) = (<em>i</em>/<em>N</em>)<sup>&alpha;</sup> and the accumulation with a weighted maximum using a 1/<em>i</em> weight. We show that this combination produces a shadow probe with <em>log-uniform effective sensitivity</em>&mdash;equal attention to each multiplicative decade of ray distance&mdash;a property that holds across the entire single-parameter family of power-law exponents. We analyze the family and show that &alpha; = 0.5 (the square-root) is the unique exponent that matches the solid-angle falloff of real occluders, as derived in Section 6.</p>

<p><strong>Scale-invariant normalization.</strong> Setting the shadow strength to N (the iteration count) exactly cancels the dependence of shadow intensity on sample count, making N a pure quality knob with no secondary parameter adjustment required.</p>

<h2>2. Motivation</h2>

<p>Modern rendering has largely moved beyond per-fragment height-field techniques. Hardware-accelerated ray tracing, mesh-based displacement via tessellation, and virtualized geometry systems such as Nanite can represent surface detail as actual geometry, with physically correct shadows emerging naturally from the scene&rsquo;s visibility solution. Physically based rendering pipelines with full material models&mdash;albedo, metallic, roughness, ambient occlusion, emissive&mdash;have become the industry standard. FXPS does not contend with any of these approaches. Where they are available, they should be preferred.</p>

<p>However, a substantial portion of real-time rendering in 2026 operates outside this frontier. Mobile games targeting OpenGL ES 2.0/3.0 or WebGL 1.0 have no access to hardware ray tracing or tessellation. Handheld gaming systems ship with GPU budgets that preclude per-pixel ray tracing at acceptable frame rates. WebGL applications must run on the intersection of all browser GPU backends, limiting available features to the common subset. Custom and legacy game engines&mdash;particularly forward-rendering pipelines&mdash;may lack the deferred infrastructure that modern shadow techniques assume. Stylized and retro-aesthetic projects deliberately choose simpler shading models for artistic reasons. And many existing asset pipelines produce only diffuse, normal, specular, and height maps, without the full PBR material suite that modern techniques require.</p>

<p>For all of these contexts, parallax occlusion mapping remains a practical tool: it adds compelling surface detail using only a height map and a few texture fetches per fragment, with no geometry amplification, no precomputed data structures, and no render-target dependencies. The self-shadowing component of POM has received comparatively little attention since Tatarchuk&rsquo;s original formulation in 2006, despite being the weakest link in the technique at low sample counts. FXPS addresses this specific gap&mdash;improving the quality of POM self-shadows in the 2&ndash;4 iteration regime where they are most needed and least served by existing work.</p>

<h3>2.1 Sample Budget Reallocation</h3>

<p>Parallax mapping quality is highly sensitive to the number of ray-march steps: more steps mean fewer missed intersections, less swimming, and sharper silhouettes. In practice, the per-fragment texture fetch budget is the binding constraint. A typical soft-shadow POM configuration might allocate 16 samples to the view-ray parallax offset and another 16 to the shadow ray, for a total budget of 32 fetches per fragment. The shadow half of that budget produces good results at 16 samples&mdash;but reducing it to 3&ndash;4 samples with FXPS frees a large block of fetches that can be spent elsewhere.</p>

<p>This creates a practical choice. One option is to ship the same visual quality at lower cost: 16 parallax samples plus 4 FXPS shadow samples is 20 fetches, a 37% reduction from the 32-fetch baseline. The other option is to reinvest the freed budget into parallax quality: 28 parallax samples plus 4 shadow samples still fits within 32 fetches, but the additional 12 parallax steps produce noticeably better surface detail&mdash;fewer silhouette artifacts, more accurate self-occlusion, and reduced texture swimming&mdash;than the original 16-step configuration could achieve. In both cases, FXPS delivers soft self-shadows that are visually comparable to the 16-sample baseline, making the shadow iteration count a net gain rather than a trade-off.</p>

<p>FXPS pairs particularly well with iterative parallax techniques such as Premecz&rsquo;s [2006] convergence method, which also produces good results at very low iteration counts. Both techniques are designed for the 3&ndash;4 sample regime: the iterative offset converges on the view-ray intersection in roughly 4 steps, and FXPS produces plausible soft shadows in 4 steps. Together, they deliver a full-featured parallax mapping system&mdash;accurate surface displacement with distance-dependent soft self-shadows&mdash;in as few as 8 texture fetches per fragment.</p>

<h2>3. Prior Work</h2>

<p>Self-shadowing of height-mapped surfaces has been explored through several families of techniques. We survey the principal approaches to parallax occlusion mapping and its self-shadowing extensions, which form the direct lineage of FXPS.</p>

<h3>3.1 Relief Mapping and Linear Ray Marching</h3>

<p>Policarpo, Oliveira, and Comba [2005] introduced relief mapping, which ray-marches through a height field in tangent space using a uniform linear search followed by a binary refinement step. This was the first technique to produce per-pixel self-occlusion and self-shadowing from height maps without precomputation, supporting fully dynamic lighting. McGuire and McGuire [2005] proposed steep parallax mapping, a simplified variant with fewer iterations. Premecz [2006] presented an iterative refinement approach that converges on the ray-surface intersection by scaling each step by the remaining height difference and the surface normal&rsquo;s Z component, providing slope-aware convergence with fewer iterations than linear search. These approaches produce hard shadows: a ray either intersects the height field (shadowed) or does not (lit), with no intermediate softness. Banding artifacts from the discrete linear search are visible at low iteration counts.</p>

<h3>3.2 Parallax Occlusion Mapping with Soft Shadows</h3>

<p>Tatarchuk [2006] extended the relief mapping framework into Parallax Occlusion Mapping (POM), adding a soft shadowing heuristic. The key innovation was replacing the binary intersection test of prior techniques with a continuous depth-difference metric: rather than asking whether the height field intersects the ray (yes or no), the technique measures <em>by how much</em> the height field exceeds the ray at each sample, producing a scalar shadow contribution. This is the fundamental reason soft POM shadows have smooth, gradual edges&mdash;at shadow boundaries, the penetration depth is genuinely small, mapping to partial shadow values through the continuous formulation. This softness is structural and holds regardless of texture filtering mode or sample count. Tatarchuk&rsquo;s technique uses a linear sampling distribution and a distance-based attenuation factor. This is the closest prior work to FXPS and shares the same structural skeleton: a loop over N samples, a depth comparison, and a distance-weighted accumulation. FXPS inherits the soft-edge property and differs in two respects: the use of a power-law (rather than linear) step distribution, and the specific 1/<em>i</em> weight (which we show produces log-uniform sensitivity).</p>

<h2>4. Power-Law Sampling and the 1/<em>i</em> Weight</h2>

<p>Tatarchuk&rsquo;s soft POM shadows use a linear distribution t(<em>i</em>) = <em>i</em>/<em>N</em>, spacing samples uniformly along the ray. We generalize this to a power-law family parameterized by exponent &alpha;:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>t</em>(<em>i</em>) = (<em>i</em> / <em>N</em>)<sup>&alpha;</sup></p>

<p>The exponent &alpha; controls where samples concentrate along the ray. Low &alpha; (e.g. 0.5, the square-root) clusters samples toward the far end, improving detection of distant macro-scale occluders. High &alpha; (e.g. 2.0 or 3.0) clusters samples near the origin, prioritizing contact shadow detection. For N = 4, the sample positions are:</p>

<table border="1" cellpadding="6" cellspacing="0">
<tr><th>&alpha;</th><th>Name</th><th>t&#8321;</th><th>t&#8322;</th><th>t&#8323;</th><th>t&#8324;</th></tr>
<tr><td>0.5</td><td>sqrt</td><td>0.500</td><td>0.707</td><td>0.866</td><td>1.000</td></tr>
<tr><td>1.0</td><td>linear</td><td>0.250</td><td>0.500</td><td>0.750</td><td>1.000</td></tr>
<tr><td>2.0</td><td>quadratic</td><td>0.063</td><td>0.250</td><td>0.563</td><td>1.000</td></tr>
<tr><td>3.0</td><td>cubic</td><td>0.016</td><td>0.125</td><td>0.422</td><td>1.000</td></tr>
</table>
<p><em>Table 1.</em> Sample positions for different power-law exponents at N = 4. Higher &alpha; concentrates samples near the ray origin; lower &alpha; pushes them toward the far end.</p>

<p>Rather than a binary occlusion test, we accumulate the shadow contribution S as a running maximum over a weighted depth difference:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>S</em> = max( <em>S</em>, (<em>H</em>_sample &minus; <em>H</em>_ray) / <em>i</em> )</p>

<p>We now characterize the <em>effective sensitivity</em> of the probe&mdash;the product of sampling density in ray-space and the weight applied at each sample.</p>

<p>For the power-law family, <em>i</em> = <em>N</em> &middot; <em>t</em><sup>1/&alpha;</sup>, giving sampling density d<em>i</em>/d<em>t</em> = (<em>N</em>/&alpha;) &middot; <em>t</em><sup>1/&alpha; &minus; 1</sup>. The weight is <em>w</em> = 1/<em>i</em> = 1/(<em>N</em> &middot; <em>t</em><sup>1/&alpha;</sup>). Their product:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&rho;(<em>t</em>) = (<em>N</em>/&alpha;) &middot; <em>t</em><sup>1/&alpha; &minus; 1</sup> &middot; 1/(<em>N</em> &middot; <em>t</em><sup>1/&alpha;</sup>) = 1/(&alpha;<em>t</em>)</p>

<p>The exponent &alpha; cancels into a constant prefactor. <strong>The 1/<em>i</em> weight produces log-uniform sensitivity for any power-law stepping function.</strong> Reparameterizing in log-space with <em>u</em> = ln(<em>t</em>), the sensitivity per unit <em>u</em> becomes &rho;(<em>t</em>) &middot; <em>t</em> = 1/&alpha;, a constant. The algorithm allocates equal attention to each multiplicative decade of distance&mdash;the interval [0.01, 0.1] receives the same total sensitivity as [0.1, 1.0].</p>

<p>[<a href="appendix.html#fig1">Figure 1</a>: Effective sensitivity &rho;(t) and sensitivity per log-decade for each power-law exponent.]</p>

<h3>4.1 Distance-Attenuation Interpretation</h3>

<p>While the log-uniform invariant holds for any &alpha;, the 1/<em>i</em> weight maps to a <em>different distance-attenuation curve</em> depending on the exponent. Since <em>i</em> = <em>N</em> &middot; <em>t</em><sup>1/&alpha;</sup>, the weight expressed in ray-space is:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;1/<em>i</em> = (1/<em>N</em>) &middot; <em>t</em><sup>&minus;1/&alpha;</sup></p>

<table border="1" cellpadding="6" cellspacing="0">
<tr><th>&alpha;</th><th>Step</th><th>Ray-space atten.</th><th>Falloff</th><th>Character</th></tr>
<tr><td>0.5</td><td>sqrt</td><td>t<sup>&minus;2</sup></td><td>Inverse square</td><td>Aggressive</td></tr>
<tr><td>1.0</td><td>linear</td><td>t<sup>&minus;1</sup></td><td>Inverse linear</td><td>Moderate</td></tr>
<tr><td>2.0</td><td>quadratic</td><td>t<sup>&minus;0.5</sup></td><td>Inverse sqrt</td><td>Gentle</td></tr>
<tr><td>3.0</td><td>cubic</td><td>t<sup>&minus;0.33</sup></td><td>Near-constant</td><td>Very gentle</td></tr>
</table>
<p><em>Table 2.</em> Distance-attenuation curves implied by the 1/i weight at different power-law exponents. As &alpha; increases, the attenuation in ray-space becomes softer.</p>

<p>[<a href="appendix.html#fig2">Figure 2</a>: Distance-attenuation curves in ray-space for each &alpha;.]</p>

<p>[<a href="appendix.html#fig3">Figure 3</a>: Discrete sample placement and weights for N = 4 at each &alpha;.]</p>

<p>[<a href="appendix.html#fig4">Figure 4</a>: Depth-difference formulation produces soft shadow edges.]</p>

<p>[<a href="appendix.html#fig5">Figure 5</a>: Distance attenuation with the 1/i weight at &alpha; = 0.5.]</p>

<p>As &alpha; increases, the attenuation in ray-space becomes softer. It is important to understand what this means in the discrete case. The 1/<em>i</em> weight assigns the same values regardless of &alpha;: {1, 1/2, 1/3, 1/4} for i = {1, 2, 3, 4}. It is a fixed index-space ramp that does not know or care about the sampling distribution. What &alpha; controls is <em>where those indices land on the ray</em>.</p>

<p>For &alpha; = 0.5, the four samples land at t = {0.50, 0.71, 0.87, 1.00}. Weight 1 is assigned to t = 0.50, weight 0.25 to t = 1.00&mdash;a 4:1 ratio across a 2:1 distance range. Projected into ray-space, this is a steep falloff. The first sample gets strongly prioritized over the last, producing a perceptual effect that <em>partially</em> resembles contact hardening across the [0.5, 1.0] band&mdash;an opacity gradient where nearby occluders cast darker shadows than distant ones. Note that this is not detecting true contact shadows&mdash;the entire [0, 0.5) range is unsampled, and no amount of weight amplification can detect an occluder at a position that was never queried.</p>

<p>For &alpha; = 2.0, the four samples land at t = {0.063, 0.25, 0.56, 1.00}. The same weight 1 is now assigned to t = 0.063, genuinely in the near field. Weight 0.25 goes to t = 1.00&mdash;a 4:1 ratio across a 16:1 distance range. Projected into ray-space, this is a much gentler curve. The first sample does not overwhelmingly dominate; the far-field samples retain meaningful influence. This is appropriate because the samples already span nearly the full ray, so the weight does not need to impose a strong spatial preference.</p>

<p>The different distance-attenuation curves are therefore not a compensatory mechanism&mdash;the weight is not &ldquo;aware&rdquo; of the sampling distribution and adapting to it. Rather, the same fixed index-space ramp is <em>projected through a different nonlinear mapping</em> depending on &alpha;. When that mapping compresses a wide ray-distance range into a narrow index range (low &alpha;), the projected curve is steep. When it stretches a wide ray-distance range across many indices (high &alpha;), the projected curve is gentle. The log-uniform effective sensitivity falls out of the <em>algebra</em> of this projection, not from any adaptive behavior in the weight itself.</p>

<p>The 1/<em>i</em> weight is the unique index-space ramp that, when projected through any power-law step function, yields log-uniform sensitivity in the continuous limit. It is a principled choice of weight&mdash;but it is a fixed formula applied uniformly, and the apparent &ldquo;adaptation&rdquo; to different &alpha; regimes is entirely a consequence of the projection geometry.</p>

<h3>4.2 Soft Shadow Edges</h3>

<p>Both FXPS and Tatarchuk&rsquo;s original technique produce spatially soft shadow edges, owing to the continuous depth-difference formulation they share (Section 3.2). The output clamp(1 &minus; <em>N</em> &middot; <em>S</em>, 0, 1) is a linear ramp, not a step function. At a shadow boundary, there is always a spatial band of fragments where the occluder just barely rises above the ray&mdash;a small positive depth difference that maps to a partial shadow value. This softness is structural: it is a property of the formulation itself and holds regardless of texture filtering mode. The width of the soft transition zone depends on the height-field gradient and the light angle, but not on occluder distance or light source extent&mdash;a distinction from true penumbrae explored further in Section 9.</p>

<h3>4.3 The Continuous-Discrete Gap</h3>

<p>An important caveat: the log-uniform property is a statement about the continuous limit. At very low iteration counts, the discrete sample placement dominates behavior. With &alpha; = 0.5 and N = 4, the first sample falls at t = 0.50&mdash;the entire [0, 0.5) range is unsampled. The four samples have weights 1, 1/2, 1/3, 1/4, forming a monotonically decreasing discrete ramp over the [0.5, 1.0] band. The visual result at low N is driven as much by this discrete weight gradient as by the continuous log-uniform property.</p>

<p>The continuous analysis nevertheless justifies the <em>choice</em> of 1/<em>i</em> as the weighting function: it is the unique weight that produces log-uniform sensitivity in the limit, and it provides a principled starting point from which the discrete behavior inherits its favorable gradient. The question becomes: which value of &alpha; makes the best use of these limited discrete samples? Section 6 addresses this question.</p>

<h2>5. Scale-Invariant Normalization</h2>

<p>For an occluder at fixed position <em>t</em>* along the ray, the nearest sample index scales as <em>i</em>* &asymp; <em>N</em> &middot; <em>t</em>*<sup>1/&alpha;</sup>. The raw shadow contribution is:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&Delta;<em>H</em> / <em>i</em>* &asymp; &Delta;<em>H</em> / (<em>N</em> &middot; <em>t</em>*<sup>1/&alpha;</sup>)</p>

<p>This is inversely proportional to N. Multiplying by strength = N cancels the dependence exactly:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>S</em>_final = <em>N</em> &middot; &Delta;<em>H</em> / (<em>N</em> &middot; <em>t</em>*<sup>1/&alpha;</sup>) = &Delta;<em>H</em> / <em>t</em>*<sup>1/&alpha;</sup></p>

<p>The shadow response becomes <strong>independent of iteration count</strong>. N can be adjusted purely as a quality knob&mdash;no secondary strength parameter is needed. Shadow intensity remains consistent across quality levels; only the spatial resolution of occluder detection changes.</p>

<p>This also explains a practical observation: setting the shadow strength above N causes banding at low sample counts. The shadow value saturates more aggressively, destroying the smooth gradient between fully lit and fully shadowed regions. With strength = N, the output stays in the linear range of the clamp, preserving the continuous falloff.</p>

<p>Crucially, this result holds for <em>any</em> &alpha; and <em>any</em> N. It is an exact discrete property, not an asymptotic one, which makes it robust in the low-iteration regime where the technique operates.</p>

<h2>6. Choosing the Exponent</h2>

<p>The analysis so far establishes a family of shadow probes parameterized by &alpha;, all sharing the log-uniform invariant and the strength = N normalization. The question remains: what value of &alpha; should be used? We show that &alpha; = 0.5 can be derived from first principles by requiring the algorithm&rsquo;s distance-attenuation curve to match the geometric falloff of real occluders.</p>

<h3>6.1 Derivation from Solid-Angle Falloff</h3>

<p>In a realistic lighting environment with ambient fill, indirect bounce, and subsurface scattering, the perceptual intensity of a shadow at a receiver point depends on the fraction of the visible hemisphere that the occluder blocks. For an occluder with projected cross-sectional area <em>A</em> at distance <em>d</em> from the receiver, the subtended solid angle is:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&Omega; &asymp; <em>A</em> / <em>d</em><sup>2</sup></p>

<p>This far-field approximation holds when <em>d</em> is sufficiently larger than the occluder. Since the ray distance <em>t</em> is directly proportional to the physical distance <em>d</em>, the physical shadow intensity attenuation follows an inverse-square law relative to <em>t</em>:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;Attenuation &prop; <em>t</em><sup>&minus;2</sup></p>

<p>The FXPS algorithm applies a weight of 1/<em>i</em> at each sample. From Section 4, the sample position is defined by the power-law function <em>t</em>(<em>i</em>) = (<em>i</em>/<em>N</em>)<sup>&alpha;</sup>. Inverting gives the sample index as a function of distance: <em>i</em> = <em>N</em> &middot; <em>t</em><sup>1/&alpha;</sup>. Substituting into the weight function gives the effective distance-attenuation curve in ray-space:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;1/<em>i</em> = (1/<em>N</em>) &middot; <em>t</em><sup>&minus;1/&alpha;</sup></p>

<p>Equating the algorithmic exponent with the physical exponent:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>t</em><sup>&minus;1/&alpha;</sup> = <em>t</em><sup>&minus;2</sup></p>

<p>gives &alpha; = 0.5. This is the unique exponent that maps the 1/<em>i</em> weight to a physically grounded <em>t</em><sup>&minus;2</sup> geometric attenuation. Lower values (e.g. &alpha; = 0.25 &rarr; <em>t</em><sup>&minus;4</sup>) over-attenuate with distance; higher values (e.g. &alpha; = 1.0 &rarr; <em>t</em><sup>&minus;1</sup>) under-attenuate. Neither matches the geometry of three-dimensional space.</p>

<h3>6.2 Validity of the Approximation</h3>

<p>The formula &Omega; &asymp; <em>A</em>/<em>d</em><sup>2</sup> assumes the occluder is small relative to the distance. The exact solid angle subtended by an arbitrary surface <em>S</em> is given by the surface integral:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&Omega; = &int;&int;<sub><em>S</em></sub> (cos&theta; / <em>r</em><sup>2</sup>) d<em>A</em></p>

<p>where <em>r</em> is the distance from the receiver to each area element and &theta; is the angle between the element&rsquo;s surface normal and the vector to the receiver. For specific geometries this resolves to closed-form expressions. For example, the exact solid angle of a circular disk of radius <em>R</em> at perpendicular distance <em>d</em> is:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&Omega; = 2&pi;(1 &minus; <em>d</em> / &radic;(<em>d</em><sup>2</sup> + <em>R</em><sup>2</sup>))</p>

<p>When <em>d</em> &Gt; <em>R</em>, the Taylor expansion recovers &pi;<em>R</em><sup>2</sup>/<em>d</em><sup>2</sup> = <em>A</em>/<em>d</em><sup>2</sup>. In the near field (<em>d</em> &asymp; <em>R</em>), the relationship saturates as the occluder subtends a large fraction of the hemisphere. &alpha; = 0.5 conveniently avoids this regime: at N = 4, the first sample falls at <em>t</em> = 0.50, where the ray has already risen to half its total excursion. The far-field approximation is most accurate precisely where the algorithm samples.</p>

<h3>6.3 Supporting Empirical Evidence</h3>

<p>Beyond the geometric derivation, several practical properties reinforce the choice of &alpha; = 0.5.</p>

<p><strong>Synergy with normal mapping.</strong> In any pipeline that uses normal maps alongside height maps, the perturbed Lambertian term &langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; already darkens fragments whose surface normal faces away from the light. This provides effective near-field self-shadowing for free: a small bump adjacent to the shading point darkens it through normal-map orientation, producing the same visual effect that a near-field shadow sample would detect. High &alpha; concentrates samples in exactly this near-field region, duplicating work that the normal map already handles. &alpha; = 0.5 avoids this redundancy by placing all samples in the far field (t &ge; 0.5 at N = 4), where it detects occluders that no amount of normal-map darkening can capture&mdash;a distant feature blocking light to a surface patch that locally faces the light.</p>

<p><strong>Near-field noise avoidance.</strong> The shadow ray starts at the surface height and rises along the light direction. At small t, the ray is barely above the surface&mdash;the geometric separation between ray and height field is minimal. In this region, minor surface variations that are not meaningful occluders can produce small positive depth differences, registering as spurious shadow contributions. High &alpha; pushes the first sample into this noisy zone (&alpha; = 2.0 places it at t = 0.063, where the ray has risen by only 6.3% of its total height excursion). &alpha; = 0.5 keeps all samples at t &ge; 0.50, where the ray has risen enough that only genuinely taller features register as occluders.</p>

<p><strong>Dense sampling where occluders reside.</strong> At typical POM scales (PARALLAX_SCALE &asymp; 0.04), the entire shadow ray is physically short. Most real shadow-casting geometry&mdash;brick edges, ridge lines, surface ledges&mdash;creates occlusion at intermediate distances along the ray, not at the extreme near field. Four samples in [0.5, 1.0] provide dense, well-separated coverage of this zone. Higher &alpha; spreads the same four samples across [0.06, 1.0], giving sparse coverage everywhere and dense coverage nowhere.</p>

<h3>6.4 Note on Height-Adaptive &alpha;</h3>

<p>We also explored a height-adaptive variant in which the exponent varies per-fragment: &alpha; = mix(&alpha;_min, &alpha;_max, <em>h</em>), where <em>h</em> is the surface height. The geometric motivation was that peaks face nearby occluders (calling for high &alpha;) while valleys face distant ones (calling for low &alpha;). While theoretically appealing, this approach did not improve results in practice. We believe this is because the fragments that receive the most visually important shadows are at mid-to-low elevation&mdash;below the features that cast shadows on them&mdash;and these fragments are precisely the ones where the adaptive scheme assigns low &alpha;, which is already the best choice regardless. Meanwhile, peak fragments receive high &alpha; but are typically shadow <em>casters</em> rather than shadow <em>receivers</em>, so the adaptive sampling is spent on fragments that are usually fully lit. Additionally, valleys can have nearby overhangs just as easily as distant rims, and the height-to-occluder-distance correlation the adaptive scheme assumes does not hold for these visually important cases. We recommend using a fixed &alpha; = 0.5 as the default.</p>

<h2>7. Notes on PBR Integration</h2>

<p>FXPS is designed as an empirical technique for stylized and performance-constrained rendering, where it can serve as a simple multiplicative darkening factor applied to the final lighting. However, it can also be inserted into a physically based pipeline if desired. This section provides guidance for that use case, characterizing how FXPS interacts with the self-shadowing mechanisms already present in a standard PBR lighting equation.</p>

<p>In a standard PBR direct lighting pipeline, the outgoing radiance at a surface point <strong>x</strong> with view direction <strong>v</strong> and light direction <strong>l</strong> is:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>L</em>_o(<strong>x</strong>, <strong>v</strong>) = <em>f</em>_r(<strong>l</strong>, <strong>v</strong>, <strong>n&prime;</strong>) &middot; <em>L</em>_i &middot; &langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; &middot; <em>V</em>(<strong>x</strong>, <strong>l</strong>)</p>

<p>where <em>f</em>_r is the BRDF, <em>L</em>_i is the incident radiance from the light, <strong>n&prime;</strong> is the perturbed (normal-mapped) surface normal, and <em>V</em> is a geometric visibility term. If FXPS is used within such a pipeline, it provides a replacement for <em>V</em>. The following subsections characterize how this interacts with the other self-shadowing mechanisms in the equation.</p>

<h3>7.1 Multi-Scale Shadow Decomposition</h3>

<p>A height-mapped PBR surface contains three distinct spatial scales of self-occlusion, each handled by a different term in the rendering equation:</p>

<p><strong>Micro-scale: the Smith shadowing-masking function G(l, v, m).</strong> Inside the Cook-Torrance specular BRDF, the geometry term <em>G</em> models the statistical self-occlusion of sub-texel microfacets as a function of roughness and the half-vector. It operates at the scale of surface microgeometry (micrometers)&mdash;far below the resolution of any height map. The specular BRDF is typically written as:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>f</em>_spec = <em>D</em> &middot; <em>F</em> &middot; <em>G</em> / (4 &langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; &langle;<strong>n&prime;</strong> &middot; <strong>v</strong>&rangle;)</p>

<p>where <em>D</em> is the microfacet distribution (e.g. GGX), <em>F</em> is the Fresnel term, and <em>G</em> encodes the proportion of microfacets that are both visible to the camera and not shadowed from the light. This is a <em>statistical</em> self-shadowing model&mdash;it does not resolve individual occluders but rather models their aggregate effect on the lobe.</p>

<p><strong>Meso-scale: the perturbed Lambertian term &langle;n&prime; &middot; l&rangle;.</strong> The clamped dot product between the normal-mapped normal <strong>n&prime;</strong> and the light direction <strong>l</strong> captures orientation-dependent darkening from surface detail encoded in the normal map. While not geometric shadowing in the strict sense, it approximates the visual effect of self-occlusion at the normal-map frequency (typically millimeter-scale surface undulations). Features that face away from the light are darkened; features facing the light receive full irradiance.</p>

<p><strong>Macro-scale: the visibility term V(x, l).</strong> This is the geometric visibility between the shading point and the light, evaluated along the height field. In standard POM, this term is either absent (no self-shadowing), binary (hard shadow), or smoothed via multi-sample averaging. FXPS replaces it with a continuous, power-law weighted probe:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>V</em>_FXPS(<strong>x</strong>, <strong>l</strong>) = clamp(1 &minus; <em>N</em> &middot; max<sub><em>i</em></sub> (<em>H</em>_sample(<em>t</em>_<em>i</em>) &minus; <em>H</em>_ray(<em>t</em>_<em>i</em>)) / <em>i</em>, 0, 1)</p>

<p>This term operates at the height-map texel scale (millimeters to centimeters)&mdash;the same frequency as the normal map, but capturing a fundamentally different phenomenon: whether a distant feature <em>occludes</em> the light, rather than whether the local surface <em>faces</em> it.</p>

<h3>7.2 The Complete Reflectance Equation</h3>

<p>Substituting the multi-scale terms, the full direct lighting equation for a single light becomes:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>L</em>_o = [<em>f</em>_spec + <em>f</em>_diff] &middot; <em>L</em>_i &middot; &langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; &middot; <em>V</em>_FXPS</p>

<p>where <em>f</em>_spec contains <em>G</em> internally and <em>f</em>_diff is the diffuse BRDF (e.g. Lambertian or Oren-Nayar). The three self-shadowing mechanisms enter multiplicatively: <em>G</em> modulates the specular lobe, &langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; modulates the total irradiance via surface orientation, and <em>V</em>_FXPS modulates the incoming radiance via macro-scale geometric occlusion.</p>

<p>The multiplicative decomposition is physically consistent because <em>V</em>_FXPS modulates <em>incoming radiance before it reaches the BRDF</em>. It is not a modification of the reflectance function itself, but of the light transport. The BRDF&rsquo;s internal <em>G</em> term then further attenuates based on microfacet statistics, operating on the already-modulated irradiance. This is consistent with the rendering equation&rsquo;s structure, where visibility and reflectance are separable.</p>

<h3>7.3 Scale Separation and Double-Counting</h3>

<p>The multiplicative composition of <em>G</em>, &langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle;, and <em>V</em>_FXPS is physically justified when they operate at non-overlapping spatial frequencies. In practice, this assumption holds well:</p>

<p>The Smith <em>G</em> function models sub-texel microgeometry at the scale of the roughness parameter (typically micrometers). POM height maps encode features at texel resolution (millimeters to centimeters). The frequency gap between these scales is typically three or more orders of magnitude, making overlap negligible.</p>

<p>The meso-scale &langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; term and the macro-scale <em>V</em>_FXPS term both operate at roughly the same spatial frequency (the height-map/normal-map texel), but they capture <em>orthogonal phenomena</em>. The dot product measures local surface orientation; <em>V</em>_FXPS measures non-local occlusion along the light ray. A surface patch can face the light (&langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; &asymp; 1) yet still be occluded by a distant ridge (<em>V</em>_FXPS &lt; 1), or face away from the light (&langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; &asymp; 0) while being geometrically unoccluded (<em>V</em>_FXPS = 1). Their product correctly captures both effects simultaneously.</p>

<p>The one scenario where mild double-counting could arise is if the height map contains features at a scale comparable to the microfacet roughness. In such a case, both <em>G</em> and <em>V</em>_FXPS would attenuate for the same geometric cause. In practice, this is extremely unlikely in real-time rendering pipelines: height maps are authored at a resolution of hundreds of texels per meter, while roughness models sub-micron geometry. Nonetheless, artists should be aware that extremely rough materials (roughness &gt; 0.9) combined with very high-frequency height maps could in principle produce slightly over-darkened results.</p>

<h3>7.4 Energy Conservation</h3>

<p><em>V</em>_FXPS is bounded to [0, 1] and is purely attenuative&mdash;it can reduce incoming radiance but never amplify it. This guarantees that the technique cannot violate energy conservation. The total outgoing radiance at any fragment is always less than or equal to the radiance that would be computed without the visibility term. In the absence of any occluders, <em>V</em>_FXPS evaluates to exactly 1.0, reducing to the unshadowed case.</p>

<p>Note that <em>V</em>_FXPS does not account for indirect illumination or interreflection within the height field. Light blocked by the visibility term is simply removed, not redistributed. In pipelines that include screen-space or probe-based global illumination, the ambient or indirect term should <em>not</em> be modulated by <em>V</em>_FXPS, as the indirect lighting has already been attenuated through its own visibility evaluation. The FXPS visibility term should be applied only to direct light contributions.</p>

<h2>8. Implementation</h2>

<p>The complete implementation in GLSL. The parallax offset function uses the iterative ray-surface convergence method of Premecz [2006], which scales each step by the height difference and the surface normal&rsquo;s Z component to attenuate on steep slopes. The shadow function uses the square-root distribution (&alpha; = 0.5) with the scale-invariant normalization. Together, the two functions require only the height map (in the alpha channel, with the normal map in RGB) and a small number of iterations.</p>

<pre><code>#define PARALLAX_SCALE 0.04
#define PARALLAX_BIAS -0.02
#define PARALLAX_OFFSET_ITERATIONS 4
#define PARALLAX_SHADOW_ITERATIONS 4

vec2 getParallaxOffset(sampler2D heightSampler, vec2 uv, vec3 eyeDir)
{
    vec3 ray = vec3(0.0);

    for (int i = 0; i &lt; PARALLAX_OFFSET_ITERATIONS; i++)
    {
        vec4 texSample = texture2D(heightSampler, uv + ray.xy);
        float sampledHeight = texSample.a * PARALLAX_SCALE + PARALLAX_BIAS;
        // Convergence driver: steps shrink as ray approaches the surface.
        float heightDiff = sampledHeight - ray.z;
        // Scale step by the surface normal's Z to attenuate on steep slopes.
        ray += eyeDir * heightDiff * texSample.z;
    }

    return ray.xy;
}

float getParallaxShadow(sampler2D heightSampler, vec2 uv, vec3 lightDir)
{
    vec3 step = lightDir * PARALLAX_SCALE;
    float surfaceHeight = texture2D(heightSampler, uv).a;

    float shadow = 0.0;
    for (int i = 1; i &lt;= PARALLAX_SHADOW_ITERATIONS; i++)
    {
        // Square-root distribution: clusters samples in far field,
        // complementing normal-map darkening in the near field
        float t = sqrt(float(i) / float(PARALLAX_SHADOW_ITERATIONS));
        float rayHeight = surfaceHeight + step.z * t;
        float sampleHeight = texture2D(heightSampler, uv + step.xy * t).a;
        // Combined with sqrt sampling, 1/i yields t^{-2} distance attenuation
        // matching solid-angle falloff of real occluders.
        shadow = max(shadow, (sampleHeight - rayHeight) / float(i));
    }

    // Using N as strength makes iteration count a pure quality parameter
    return clamp(1.0 - shadow * float(PARALLAX_SHADOW_ITERATIONS), 0.0, 1.0);
}</code></pre>

<p>The shadow function requires exactly N texture fetches plus one for the surface height, identical to standard hard-shadow POM. The offset function requires one texture fetch per iteration. The additional ALU cost over linear POM is a single sqrt in the shadow function. No additional render targets, temporal accumulation, or stochastic sampling are required.</p>

<h2>9. Limitations and Future Work</h2>

<p>The derivation of &alpha; = 0.5 from solid-angle falloff (Section 6.1) relies on the far-field approximation &Omega; &asymp; <em>A</em>/<em>d</em><sup>2</sup>, which assumes the occluder is small relative to its distance from the receiver. For height fields with very large-scale features where this assumption breaks down, other values of &alpha; may produce better results. Additionally, the derivation assumes a realistic multi-source lighting environment where shadow intensity correlates with subtended solid angle; in scenes lit by a single infinitely distant directional light with no ambient contribution, the physical motivation is weaker. A systematic evaluation across a wider range of height-map types and lighting configurations would clarify the boundaries of applicability.</p>

<p>At N = 2, the technique still produces usable results with subtle banding, but the probe has very limited spatial discrimination. Performance profiling across GPU architectures, particularly regarding texture cache coherence of the non-linear sampling pattern, would clarify the practical cost-quality trade-off.</p>

<p>The technique approximates the <em>opacity</em> component of contact hardening&mdash;nearby occluders produce darker shadows than distant ones, owing to the 1/<em>i</em> weight falloff&mdash;and produces genuinely soft shadow edges through the continuous depth-difference formulation. Because the output is a linear ramp (not a binary threshold), any region where the occluder just barely rises above the ray maps to a partial shadow, creating a gradual spatial transition from lit to shadowed. This softness is structural&mdash;it is a property of the formulation itself and holds regardless of texture filtering mode. However, the edge width is governed by the local height-field gradient and the light angle, not by occluder distance or light source extent. In real penumbrae, shadow edges widen with distance from the occluder; in FXPS, a distant occluder and a nearby occluder with the same height-field gradient profile produce edges of the same width. The technique therefore captures two of the three components of realistic contact hardening (opacity falloff and spatial softness) but not the third (distance-dependent penumbra widening).</p>

<p>Future work could investigate extending the 1/<em>i</em> invariant to non-power-law step functions with appropriately derived weights, or combining FXPS with screen-space techniques that could provide the missing distance-dependent edge widening.</p>

<h2>10. Conclusion</h2>

<p>We have presented Fast Approximate Parallax Shadows (FXPS), a set of improvements to Tatarchuk&rsquo;s [2006] soft POM shadow technique targeting the low-iteration regime. Like the original, FXPS is an empirical heuristic that does not aspire to physical accuracy; it is designed for the bottom of the cost spectrum&mdash;stylized rendering, mobile GPUs, legacy forward pipelines, and any context where visual plausibility at 2&ndash;4 texture fetches matters more than ground-truth visibility.</p>

<p>The improvements are built on two results: first, that generalizing the linear sampling distribution to a power-law t(<em>i</em>) = (<em>i</em>/<em>N</em>)<sup>&alpha;</sup> and combining it with a 1/<em>i</em> weight produces a shadow probe with log-uniform effective sensitivity along the ray, providing a principled basis for the weighting choice; and second, that the correct shadow strength normalization is simply N, making iteration count a pure quality knob independent of all other parameters. We showed that &alpha; = 0.5 (the square-root distribution) is the unique exponent whose distance-attenuation curve matches the solid-angle falloff of real occluders&mdash;the <em>t</em><sup>&minus;2</sup> relationship governing how much of the visible hemisphere an occluder blocks as a function of distance. This choice is further supported by its synergy with normal-map darkening, avoidance of near-field noise, and dense sampling of the region where real occluders reside.</p>

<p>Against the realistic baseline comparison&mdash;Tatarchuk&rsquo;s original linear soft shadows, hard POM shadows, simple N&middot;L darkening, or no self-shadowing at all&mdash;FXPS offers a meaningful visual improvement for the same texture fetch budget. It requires no precomputed data, no temporal accumulation, and no parameter tuning. For pipelines that need self-shadowing to look good rather than be correct, it is a practical and effective tool.</p>

</body>
</html>
