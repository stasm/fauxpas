<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Fast Approximate Parallax Shadows</title>
</head>
<body>

<h1>Fast Approximate Parallax Shadows</h1>
<p><em>Principled Self-Shadowing for Height-Mapped Surfaces at Minimal Sample Counts</em></p>

<h2>Abstract</h2>

<p>We present Fast Approximate Parallax Shadows (FXPS), a set of improvements to Tatarchuk&rsquo;s [2006] soft POM shadow technique that target the low-iteration regime (N = 2&ndash;4) where linear sampling breaks down. Like the original, FXPS is an empirical heuristic designed for contexts where visual plausibility matters more than physical accuracy&mdash;stylized rendering, mobile GPUs, legacy pipelines, and performance-constrained material layers. It replaces the linear sampling distribution with a power-law function and the accumulation with a 1/<em>i</em> weighted maximum, producing a shadow probe whose effective sensitivity is <em>log-uniform</em> along the ray&mdash;equal attention is allocated to each multiplicative decade of distance. We show that this property is invariant across a single-parameter family of power-law exponents &alpha;, and that the correct shadow strength normalization is simply N (the iteration count), making sample count a pure quality knob.</p>

<p>We analyze the behavior of the power-law family and show that &alpha; = 0.5 (the square-root distribution) is the unique exponent whose distance-attenuation curve matches the solid-angle falloff governing how much of the visible hemisphere an occluder blocks as a function of distance&mdash;the <em>t</em><sup>&minus;2</sup> relationship. Evaluating this hemispherical occlusion metric along a specific light vector produces a <em>directional occlusion</em> term: a hybrid between ambient occlusion (which integrates over the full hemisphere but is directionless) and a hard shadow test (which queries a single ray but is binary). The result is light-aware, distance-dependent shadow opacity with high-frequency contrast that visually reinforces the volume of the height-map geometry, and spatially soft shadow edges arising structurally from the continuous depth-difference formulation&mdash;at 2&ndash;4 iterations per fragment, a meaningful visual improvement over Tatarchuk&rsquo;s linear soft shadows or no self-shadowing at all, for the same texture fetch budget.</p>

<h2>1. Introduction</h2>

<p>Tatarchuk [2006] introduced soft self-shadowing for Parallax Occlusion Mapping (POM) by replacing the binary intersection test of earlier techniques with a continuous depth-difference metric. This was a significant advance: rather than asking whether the height field intersects the light ray (producing hard, banded edges), the technique measures <em>by how much</em> each sample exceeds the ray, producing smooth shadow gradients that look plausible even at moderate sample counts. Her approach uses a linear distribution of samples along the ray, t(<em>i</em>) = <em>i</em>/<em>N</em>, with a distance-based attenuation factor to weight each sample&rsquo;s contribution.</p>

<p>At high iteration counts, this works well. At the very low sample counts demanded by mobile GPUs, legacy pipelines, and performance-constrained material layers (N = 2&ndash;4), however, the linear sampling strategy has two weaknesses. First, it distributes samples uniformly along the ray, wasting budget on regions that may be irrelevant to the local shadow topology. Second, it provides no natural mechanism for contact hardening&mdash;the perceptual property whereby shadows are darkest and sharpest near occluder-receiver contact, becoming lighter and wider with distance.</p>

<p>We present Fast Approximate Parallax Shadows (FXPS), a set of improvements to Tatarchuk&rsquo;s soft POM shadow technique that target the low-iteration regime. Like the original, FXPS is an empirical heuristic&mdash;not a physically based visibility computation&mdash;and is designed for contexts where visual plausibility matters more than physical accuracy. The improvements are:</p>

<p><strong>Power-law sampling with 1/<em>i</em> weighting.</strong> We replace the linear step distribution with a power-law function t(<em>i</em>) = (<em>i</em>/<em>N</em>)<sup>&alpha;</sup> and the accumulation with a weighted maximum using a 1/<em>i</em> weight. We show that this combination produces a shadow probe with <em>log-uniform effective sensitivity</em>&mdash;equal attention to each multiplicative decade of ray distance&mdash;a property that holds across the entire single-parameter family of power-law exponents. We analyze the family and show that &alpha; = 0.5 (the square-root) is the unique exponent whose distance-attenuation curve matches the solid-angle falloff governing how much of the visible hemisphere an occluder subtends&mdash;the geometric <em>t</em><sup>&minus;2</sup> relationship. Evaluating this along the light vector produces a directional occlusion term that is light-aware and distance-dependent, as derived in Section 6.</p>

<p><strong>Scale-invariant normalization.</strong> Setting the shadow strength to N (the iteration count) exactly cancels the dependence of shadow intensity on sample count, making N a pure quality knob with no secondary parameter adjustment required.</p>

<h2>2. Motivation</h2>

<p>Modern rendering has largely moved beyond per-fragment height-field techniques. Hardware-accelerated ray tracing, mesh-based displacement via tessellation, and virtualized geometry systems such as Nanite can represent surface detail as actual geometry, with physically correct shadows emerging naturally from the scene&rsquo;s visibility solution. Physically based rendering pipelines with full material models&mdash;albedo, metallic, roughness, ambient occlusion, emissive&mdash;have become the industry standard. FXPS does not contend with any of these approaches. Where they are available, they should be preferred.</p>

<p>However, a substantial portion of real-time rendering in 2026 operates outside this frontier. Mobile games targeting OpenGL ES 2.0/3.0 or WebGL 1.0 have no access to hardware ray tracing or tessellation. Handheld gaming systems ship with GPU budgets that preclude per-pixel ray tracing at acceptable frame rates. WebGL applications must run on the intersection of all browser GPU backends, limiting available features to the common subset. Custom and legacy game engines&mdash;particularly forward-rendering pipelines&mdash;may lack the deferred infrastructure that modern shadow techniques assume. Stylized and retro-aesthetic projects deliberately choose simpler shading models for artistic reasons. And many existing asset pipelines produce only diffuse, normal, specular, and height maps, without the full PBR material suite that modern techniques require.</p>

<p>For all of these contexts, parallax occlusion mapping remains a practical tool: it adds compelling surface detail using only a height map and a few texture fetches per fragment, with no geometry amplification, no precomputed data structures, and no render-target dependencies. The self-shadowing component of POM has received comparatively little attention since Tatarchuk&rsquo;s original formulation in 2006, despite being the weakest link in the technique at low sample counts. FXPS addresses this specific gap&mdash;the meso/macro scale in the table below&mdash;improving the quality of POM self-shadows in the 2&ndash;4 iteration regime where they are most needed and least served by existing work.</p>

<table border="1" cellpadding="6" cellspacing="0">
<thead>
<tr><th>Scale</th><th>Technique</th></tr>
</thead>
<tbody>
<tr><td>Micro</td><td>Microfacet BRDF shadowing (Smith <em>G</em>)</td></tr>
<tr><td>Meso</td><td>Normal mapping</td></tr>
<tr><td>Meso/Macro</td><td>POM / cone stepping &mdash; <strong>FXPS operates here</strong></td></tr>
<tr><td>Macro</td><td>Screen-space shadows</td></tr>
<tr><td>Scene</td><td>Shadow maps / ray-traced shadows</td></tr>
</tbody>
</table>

<h3>2.1 Sample Budget Reallocation</h3>

<p>Parallax mapping quality is highly sensitive to the number of ray-march steps: more steps mean fewer missed intersections, less swimming, and sharper silhouettes. In practice, the per-fragment texture fetch budget is the binding constraint. A typical soft-shadow POM configuration might allocate 16 samples to the view-ray parallax offset and another 16 to the shadow ray, for a total budget of 32 fetches per fragment. The shadow half of that budget produces good results at 16 samples&mdash;but reducing it to 3&ndash;4 samples with FXPS frees a large block of fetches that can be spent elsewhere.</p>

<p>This creates a practical choice. One option is to ship the same visual quality at lower cost: 16 parallax samples plus 4 FXPS shadow samples is 20 fetches, a 37% reduction from the 32-fetch baseline. The other option is to reinvest the freed budget into parallax quality: 28 parallax samples plus 4 shadow samples still fits within 32 fetches, but the additional 12 parallax steps produce noticeably better surface detail&mdash;fewer silhouette artifacts, more accurate self-occlusion, and reduced texture swimming&mdash;than the original 16-step configuration could achieve. In both cases, FXPS delivers soft self-shadows that are visually comparable to the 16-sample baseline, making the shadow iteration count a net gain rather than a trade-off.</p>

<p>FXPS pairs particularly well with iterative parallax techniques such as Premecz&rsquo;s [2006] convergence method, which also produces good results at very low iteration counts. Both techniques are designed for the 3&ndash;4 sample regime: the iterative offset converges on the view-ray intersection in roughly 4 steps, and FXPS produces plausible soft shadows in 4 steps. Together, they deliver a full-featured parallax mapping system&mdash;accurate surface displacement with distance-dependent soft self-shadows&mdash;in as few as 8 texture fetches per fragment.</p>

<h2>3. Prior Work</h2>

<p>Self-shadowing of height-mapped surfaces has been explored through several families of techniques. We survey the principal approaches to parallax occlusion mapping and its self-shadowing extensions, which form the direct lineage of FXPS.</p>

<h3>3.1 Relief Mapping and Linear Ray Marching</h3>

<p>Policarpo, Oliveira, and Comba [2005] introduced relief mapping, which ray-marches through a height field in tangent space using a uniform linear search followed by a binary refinement step. This was the first technique to produce per-pixel self-occlusion and self-shadowing from height maps without precomputation, supporting fully dynamic lighting. McGuire and McGuire [2005] proposed steep parallax mapping, a simplified variant with fewer iterations. Premecz [2006] presented an iterative refinement approach that converges on the ray-surface intersection by scaling each step by the remaining height difference and the surface normal&rsquo;s Z component, providing slope-aware convergence with fewer iterations than linear search. These approaches produce hard shadows: a ray either intersects the height field (shadowed) or does not (lit), with no intermediate softness. Banding artifacts from the discrete linear search are visible at low iteration counts.</p>

<h3>3.2 Parallax Occlusion Mapping with Soft Shadows</h3>

<p>Tatarchuk [2006] extended the relief mapping framework into Parallax Occlusion Mapping (POM), adding a soft shadowing heuristic. The key innovation was replacing the binary intersection test of prior techniques with a continuous depth-difference metric: rather than asking whether the height field intersects the ray (yes or no), the technique measures <em>by how much</em> the height field exceeds the ray at each sample, producing a scalar shadow contribution. This is the fundamental reason soft POM shadows have smooth, gradual edges&mdash;at shadow boundaries, the penetration depth is genuinely small, mapping to partial shadow values through the continuous formulation. This softness is structural and holds regardless of texture filtering mode or sample count. Tatarchuk&rsquo;s technique uses a linear sampling distribution and a distance-based attenuation factor. This is the closest prior work to FXPS and shares the same structural skeleton: a loop over N samples, a depth comparison, and a distance-weighted accumulation. FXPS inherits the soft-edge property and differs in two respects: the use of a power-law (rather than linear) step distribution, and the specific 1/<em>i</em> weight (which we show produces log-uniform sensitivity).</p>

<h2>4. Power-Law Sampling and the 1/<em>i</em> Weight</h2>

<p>Tatarchuk&rsquo;s soft POM shadows use a linear distribution t(<em>i</em>) = <em>i</em>/<em>N</em>, spacing samples uniformly along the ray. We generalize this to a power-law family parameterized by exponent &alpha;:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>t</em>(<em>i</em>) = (<em>i</em> / <em>N</em>)<sup>&alpha;</sup></p>

<p>The exponent &alpha; controls where samples concentrate along the ray. Low &alpha; (e.g. 0.5, the square-root) clusters samples toward the far end, improving detection of distant macro-scale occluders. High &alpha; (e.g. 2.0 or 3.0) clusters samples near the origin, prioritizing contact shadow detection. For N = 4, the sample positions are:</p>

<table border="1" cellpadding="6" cellspacing="0">
<tr><th>&alpha;</th><th>Name</th><th>t&#8321;</th><th>t&#8322;</th><th>t&#8323;</th><th>t&#8324;</th></tr>
<tr><td>0.5</td><td>sqrt</td><td>0.500</td><td>0.707</td><td>0.866</td><td>1.000</td></tr>
<tr><td>1.0</td><td>linear</td><td>0.250</td><td>0.500</td><td>0.750</td><td>1.000</td></tr>
<tr><td>2.0</td><td>quadratic</td><td>0.063</td><td>0.250</td><td>0.563</td><td>1.000</td></tr>
<tr><td>3.0</td><td>cubic</td><td>0.016</td><td>0.125</td><td>0.422</td><td>1.000</td></tr>
</table>
<p><em>Table 1.</em> Sample positions for different power-law exponents at N = 4. Higher &alpha; concentrates samples near the ray origin; lower &alpha; pushes them toward the far end.</p>

<p>Rather than a binary occlusion test, we accumulate the shadow contribution S as a running maximum over a weighted depth difference:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>S</em> = max( <em>S</em>, (<em>H</em>_sample &minus; <em>H</em>_ray) / <em>i</em> )</p>

<p>We now characterize the <em>effective sensitivity</em> of the probe&mdash;the product of sampling density in ray-space and the weight applied at each sample.</p>

<p>For the power-law family, <em>i</em> = <em>N</em> &middot; <em>t</em><sup>1/&alpha;</sup>, giving sampling density d<em>i</em>/d<em>t</em> = (<em>N</em>/&alpha;) &middot; <em>t</em><sup>1/&alpha; &minus; 1</sup>. The weight is <em>w</em> = 1/<em>i</em> = 1/(<em>N</em> &middot; <em>t</em><sup>1/&alpha;</sup>). Their product:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&rho;(<em>t</em>) = (<em>N</em>/&alpha;) &middot; <em>t</em><sup>1/&alpha; &minus; 1</sup> &middot; 1/(<em>N</em> &middot; <em>t</em><sup>1/&alpha;</sup>) = 1/(&alpha;<em>t</em>)</p>

<p>The exponent &alpha; cancels into a constant prefactor. <strong>The 1/<em>i</em> weight produces log-uniform sensitivity for any power-law stepping function.</strong> Reparameterizing in log-space with <em>u</em> = ln(<em>t</em>), the sensitivity per unit <em>u</em> becomes &rho;(<em>t</em>) &middot; <em>t</em> = 1/&alpha;, a constant. The algorithm allocates equal attention to each multiplicative decade of distance&mdash;the interval [0.01, 0.1] receives the same total sensitivity as [0.1, 1.0].</p>

<p>[<a href="appendix.html#fig1">Figure 1</a>: Effective sensitivity &rho;(t) and sensitivity per log-decade for each power-law exponent.]</p>

<h3>4.1 Distance-Attenuation Interpretation</h3>

<p>While the log-uniform invariant holds for any &alpha;, the 1/<em>i</em> weight maps to a <em>different distance-attenuation curve</em> depending on the exponent. Since <em>i</em> = <em>N</em> &middot; <em>t</em><sup>1/&alpha;</sup>, the weight expressed in ray-space is:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;1/<em>i</em> = (1/<em>N</em>) &middot; <em>t</em><sup>&minus;1/&alpha;</sup></p>

<table border="1" cellpadding="6" cellspacing="0">
<tr><th>&alpha;</th><th>Step</th><th>Ray-space atten.</th><th>Falloff</th><th>Character</th></tr>
<tr><td>0.5</td><td>sqrt</td><td>t<sup>&minus;2</sup></td><td>Inverse square</td><td>Aggressive</td></tr>
<tr><td>1.0</td><td>linear</td><td>t<sup>&minus;1</sup></td><td>Inverse linear</td><td>Moderate</td></tr>
<tr><td>2.0</td><td>quadratic</td><td>t<sup>&minus;0.5</sup></td><td>Inverse sqrt</td><td>Gentle</td></tr>
<tr><td>3.0</td><td>cubic</td><td>t<sup>&minus;0.33</sup></td><td>Near-constant</td><td>Very gentle</td></tr>
</table>
<p><em>Table 2.</em> Distance-attenuation curves implied by the 1/i weight at different power-law exponents. Only &alpha; = 0.5 recovers the t<sup>&minus;2</sup> solid-angle falloff of hemispherical occlusion. As &alpha; increases, the attenuation in ray-space becomes softer.</p>

<p>[<a href="appendix.html#fig2">Figure 2</a>: Distance-attenuation curves in ray-space for each &alpha;.]</p>

<p>[<a href="appendix.html#fig3">Figure 3</a>: Discrete sample placement and weights for N = 4 at each &alpha;.]</p>

<p>[<a href="appendix.html#fig4">Figure 4</a>: Depth-difference formulation produces soft shadow edges.]</p>

<p>[<a href="appendix.html#fig5">Figure 5</a>: Distance attenuation with the 1/i weight at &alpha; = 0.5.]</p>

<p>As &alpha; increases, the attenuation in ray-space becomes softer. It is important to understand what this means in the discrete case. The 1/<em>i</em> weight assigns the same values regardless of &alpha;: {1, 1/2, 1/3, 1/4} for i = {1, 2, 3, 4}. It is a fixed index-space ramp that does not know or care about the sampling distribution. What &alpha; controls is <em>where those indices land on the ray</em>.</p>

<p>For &alpha; = 0.5, the four samples land at t = {0.50, 0.71, 0.87, 1.00}. Weight 1 is assigned to t = 0.50, weight 0.25 to t = 1.00&mdash;a 4:1 ratio across a 2:1 distance range. Projected into ray-space, this is a steep falloff. The first sample gets strongly prioritized over the last, producing a perceptual effect that <em>partially</em> resembles contact hardening across the [0.5, 1.0] band&mdash;an opacity gradient where nearby occluders cast darker shadows than distant ones. Note that this is not detecting true contact shadows&mdash;the entire [0, 0.5) range is unsampled, and no amount of weight amplification can detect an occluder at a position that was never queried.</p>

<p>For &alpha; = 2.0, the four samples land at t = {0.063, 0.25, 0.56, 1.00}. The same weight 1 is now assigned to t = 0.063, genuinely in the near field. Weight 0.25 goes to t = 1.00&mdash;a 4:1 ratio across a 16:1 distance range. Projected into ray-space, this is a much gentler curve. The first sample does not overwhelmingly dominate; the far-field samples retain meaningful influence. This is appropriate because the samples already span nearly the full ray, so the weight does not need to impose a strong spatial preference.</p>

<p>The different distance-attenuation curves are therefore not a compensatory mechanism&mdash;the weight is not &ldquo;aware&rdquo; of the sampling distribution and adapting to it. Rather, the same fixed index-space ramp is <em>projected through a different nonlinear mapping</em> depending on &alpha;. When that mapping compresses a wide ray-distance range into a narrow index range (low &alpha;), the projected curve is steep. When it stretches a wide ray-distance range across many indices (high &alpha;), the projected curve is gentle. The log-uniform effective sensitivity falls out of the <em>algebra</em> of this projection, not from any adaptive behavior in the weight itself.</p>

<p>The 1/<em>i</em> weight is the unique index-space ramp that, when projected through any power-law step function, yields log-uniform sensitivity in the continuous limit. It is a principled choice of weight&mdash;but it is a fixed formula applied uniformly, and the apparent &ldquo;adaptation&rdquo; to different &alpha; regimes is entirely a consequence of the projection geometry.</p>

<h3>4.2 Soft Shadow Edges</h3>

<p>Both FXPS and Tatarchuk&rsquo;s original technique produce spatially soft shadow edges, owing to the continuous depth-difference formulation they share (Section 3.2). The output clamp(1 &minus; <em>N</em> &middot; <em>S</em>, 0, 1) is a linear ramp, not a step function. At a shadow boundary, there is always a spatial band of fragments where the occluder just barely rises above the ray&mdash;a small positive depth difference that maps to a partial shadow value. This softness is structural: it is a property of the formulation itself and holds regardless of texture filtering mode. The width of the soft transition zone depends on the height-field gradient and the light angle, but not on occluder distance or light source extent&mdash;a distinction from true penumbrae explored further in Section 8.</p>

<h3>4.3 The Continuous-Discrete Gap</h3>

<p>An important caveat: the log-uniform property is a statement about the continuous limit. At very low iteration counts, the discrete sample placement dominates behavior. With &alpha; = 0.5 and N = 4, the first sample falls at t = 0.50&mdash;the entire [0, 0.5) range is unsampled. The four samples have weights 1, 1/2, 1/3, 1/4, forming a monotonically decreasing discrete ramp over the [0.5, 1.0] band. The visual result at low N is driven as much by this discrete weight gradient as by the continuous log-uniform property.</p>

<p>The continuous analysis nevertheless justifies the <em>choice</em> of 1/<em>i</em> as the weighting function: it is the unique weight that produces log-uniform sensitivity in the limit, and it provides a principled starting point from which the discrete behavior inherits its favorable gradient. The question becomes: which value of &alpha; makes the best use of these limited discrete samples? Section 6 addresses this question.</p>

<h2>5. Scale-Invariant Normalization</h2>

<p>For an occluder at fixed position <em>t</em>* along the ray, the nearest sample index scales as <em>i</em>* &asymp; <em>N</em> &middot; <em>t</em>*<sup>1/&alpha;</sup>. The raw shadow contribution is:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&Delta;<em>H</em> / <em>i</em>* &asymp; &Delta;<em>H</em> / (<em>N</em> &middot; <em>t</em>*<sup>1/&alpha;</sup>)</p>

<p>This is inversely proportional to N. Multiplying by strength = N cancels the dependence exactly:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>S</em>_final = <em>N</em> &middot; &Delta;<em>H</em> / (<em>N</em> &middot; <em>t</em>*<sup>1/&alpha;</sup>) = &Delta;<em>H</em> / <em>t</em>*<sup>1/&alpha;</sup></p>

<p>The shadow response becomes <strong>independent of iteration count</strong>. N can be adjusted purely as a quality knob&mdash;no secondary strength parameter is needed. Shadow intensity remains consistent across quality levels; only the spatial resolution of occluder detection changes.</p>

<p>This also explains a practical observation: setting the shadow strength above N causes banding at low sample counts. The shadow value saturates more aggressively, destroying the smooth gradient between fully lit and fully shadowed regions. With strength = N, the output stays in the linear range of the clamp, preserving the continuous falloff.</p>

<p>Crucially, this result holds for <em>any</em> &alpha; and <em>any</em> N. It is an exact discrete property, not an asymptotic one, which makes it robust in the low-iteration regime where the technique operates.</p>

<h2>6. Choosing the Exponent</h2>

<p>The analysis so far establishes a family of shadow probes parameterized by &alpha;, all sharing the log-uniform invariant and the strength = N normalization. The question remains: what value of &alpha; should be used? We show that &alpha; = 0.5 can be derived from first principles by requiring the algorithm&rsquo;s distance-attenuation curve to match the rate at which an occluder&rsquo;s subtended solid angle shrinks with distance&mdash;the geometric relationship governing how much of the visible hemisphere an object blocks.</p>

<h3>6.1 Derivation from Solid-Angle Falloff</h3>

<p>The inverse-square law describes how much of the visible hemisphere an occluder blocks as a function of distance. For an occluder with projected cross-sectional area <em>A</em> at distance <em>d</em> from the receiver, the subtended solid angle is:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&Omega; &asymp; <em>A</em> / <em>d</em><sup>2</sup></p>

<p>This is a statement about hemispherical coverage&mdash;the fraction of the hemisphere that the occluder subtends. It governs ambient occlusion, where the shadow intensity at a point depends on how much of the omnidirectional light field is blocked by nearby geometry. It does not, strictly speaking, describe the visibility along a single directional light ray: a directional shadow is a binary event (the occluder either intersects the light vector or it does not), and the occluder&rsquo;s solid angle at the receiver is irrelevant to that binary query.</p>

<p>However, FXPS evaluates this hemispherical-occlusion metric <em>along a specific light vector</em>. The result is neither ambient occlusion (which integrates over the full hemisphere but is directionless) nor a hard shadow test (which queries a single ray but is binary). It is a <em>directional occlusion</em> term: a distance-dependent opacity that measures how strongly a feature along the light direction would block the surrounding light environment. Near occluders subtend a large solid angle and cast dark shadows; distant occluders subtend less of the hemisphere and cast proportionally lighter ones. Crucially, this opacity is evaluated only in the direction the light comes from, so the darkening is concentrated where the main light struggles to reach&mdash;producing high-frequency, light-aware contrast rather than the uniform cavity darkening of standard ambient occlusion.</p>

<p>This directional-occlusion concept has precedent in production rendering. Standard ambient occlusion flattens surface detail by uniformly darkening crevices regardless of light direction. To counteract this, modern engines compute <em>directional occlusion</em> or <em>bent normals</em>&mdash;biasing the AO term toward the dominant light vector so that darkening is concentrated where the main light is actually blocked, preserving contrast and preventing the lighting from looking excessively flat. FXPS arrives at the same concept from a different starting point: rather than computing full hemispherical AO and then biasing it toward the light, it evaluates the hemispherical-occlusion distance metric directly along the light vector, achieving a similar light-aware darkening in a single pass at minimal cost.</p>

<p>This interpretation is most physically motivated when the lighting environment includes ambient fill, indirect bounce, or skylight in addition to the primary light source, so that the shadow intensity genuinely correlates with the hemispherical solid angle blocked. In scenes lit by a single infinitely distant directional light with no ambient contribution, the solid-angle motivation is weaker (see Section 8). In practice, even minimal ambient or fill light is sufficient for the perceptual effect to hold.</p>

<p>Since the ray distance <em>t</em> is directly proportional to the physical distance <em>d</em>, the desired distance-attenuation curve is:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;Attenuation &prop; <em>t</em><sup>&minus;2</sup></p>

<p>The FXPS algorithm applies a weight of 1/<em>i</em> at each sample. From Section 4, the sample position is defined by the power-law function <em>t</em>(<em>i</em>) = (<em>i</em>/<em>N</em>)<sup>&alpha;</sup>. Inverting gives the sample index as a function of distance: <em>i</em> = <em>N</em> &middot; <em>t</em><sup>1/&alpha;</sup>. Substituting into the weight function gives the effective distance-attenuation curve in ray-space:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;1/<em>i</em> = (1/<em>N</em>) &middot; <em>t</em><sup>&minus;1/&alpha;</sup></p>

<p>Equating the algorithmic exponent with the physical exponent:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>t</em><sup>&minus;1/&alpha;</sup> = <em>t</em><sup>&minus;2</sup></p>

<p>gives &alpha; = 0.5. This is the unique exponent that maps the 1/<em>i</em> weight to the <em>t</em><sup>&minus;2</sup> solid-angle falloff of hemispherical occlusion. At this exponent, the algorithm&rsquo;s directional-occlusion term attenuates with distance at the same rate that an occluder&rsquo;s subtended solid angle shrinks&mdash;grounding the shadow opacity in the geometry of three-dimensional space. Lower values (e.g. &alpha; = 0.25 &rarr; <em>t</em><sup>&minus;4</sup>) over-attenuate with distance; higher values (e.g. &alpha; = 1.0 &rarr; <em>t</em><sup>&minus;1</sup>) under-attenuate. Neither matches the rate at which solid angle diminishes.</p>

<h3>6.2 Validity of the Approximation</h3>

<p>The formula &Omega; &asymp; <em>A</em>/<em>d</em><sup>2</sup> assumes the occluder is small relative to the distance. The exact solid angle subtended by an arbitrary surface <em>S</em> is given by the surface integral:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&Omega; = &int;&int;<sub><em>S</em></sub> (cos&theta; / <em>r</em><sup>2</sup>) d<em>A</em></p>

<p>where <em>r</em> is the distance from the receiver to each area element and &theta; is the angle between the element&rsquo;s surface normal and the vector to the receiver. For specific geometries this resolves to closed-form expressions. For example, the exact solid angle of a circular disk of radius <em>R</em> at perpendicular distance <em>d</em> is:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&Omega; = 2&pi;(1 &minus; <em>d</em> / &radic;(<em>d</em><sup>2</sup> + <em>R</em><sup>2</sup>))</p>

<p>When <em>d</em> &Gt; <em>R</em>, the Taylor expansion recovers &pi;<em>R</em><sup>2</sup>/<em>d</em><sup>2</sup> = <em>A</em>/<em>d</em><sup>2</sup>. In the near field (<em>d</em> &asymp; <em>R</em>), the relationship saturates as the occluder subtends a large fraction of the hemisphere. &alpha; = 0.5 conveniently avoids this regime: at N = 4, the first sample falls at <em>t</em> = 0.50, where the ray has already risen to half its total excursion. The far-field approximation&mdash;and therefore the directional-occlusion interpretation it motivates&mdash;is most accurate precisely where the algorithm samples.</p>

<h3>6.3 Supporting Empirical Evidence</h3>

<p>Beyond the geometric derivation, several practical properties reinforce the choice of &alpha; = 0.5.</p>

<p><strong>Synergy with normal mapping.</strong> In any pipeline that uses normal maps alongside height maps, the perturbed Lambertian term &langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; already darkens fragments whose surface normal faces away from the light. This provides effective near-field self-shadowing for free: a small bump adjacent to the shading point darkens it through normal-map orientation, producing the same visual effect that a near-field shadow sample would detect. High &alpha; concentrates samples in exactly this near-field region, duplicating work that the normal map already handles. &alpha; = 0.5 avoids this redundancy by placing all samples in the far field (t &ge; 0.5 at N = 4), where it detects occluders that no amount of normal-map darkening can capture&mdash;a distant feature blocking light to a surface patch that locally faces the light.</p>

<p><strong>Near-field noise avoidance.</strong> The shadow ray starts at the surface height and rises along the light direction. At small t, the ray is barely above the surface&mdash;the geometric separation between ray and height field is minimal. In this region, minor surface variations that are not meaningful occluders can produce small positive depth differences, registering as spurious shadow contributions. High &alpha; pushes the first sample into this noisy zone (&alpha; = 2.0 places it at t = 0.063, where the ray has risen by only 6.3% of its total height excursion). &alpha; = 0.5 keeps all samples at t &ge; 0.50, where the ray has risen enough that only genuinely taller features register as occluders.</p>

<p><strong>Dense sampling where occluders reside.</strong> At typical POM scales (PARALLAX_SCALE &asymp; 0.04), the entire shadow ray is physically short. Most real shadow-casting geometry&mdash;brick edges, ridge lines, surface ledges&mdash;creates occlusion at intermediate distances along the ray, not at the extreme near field. Four samples in [0.5, 1.0] provide dense, well-separated coverage of this zone. Higher &alpha; spreads the same four samples across [0.06, 1.0], giving sparse coverage everywhere and dense coverage nowhere.</p>

<h3>6.4 Note on Height-Adaptive &alpha;</h3>

<p>We also explored a height-adaptive variant in which the exponent varies per-fragment: &alpha; = mix(&alpha;_min, &alpha;_max, <em>h</em>), where <em>h</em> is the surface height. The geometric motivation was that peaks face nearby occluders (calling for high &alpha;) while valleys face distant ones (calling for low &alpha;). While theoretically appealing, this approach did not improve results in practice. We believe this is because the fragments that receive the most visually important shadows are at mid-to-low elevation&mdash;below the features that cast shadows on them&mdash;and these fragments are precisely the ones where the adaptive scheme assigns low &alpha;, which is already the best choice regardless. Meanwhile, peak fragments receive high &alpha; but are typically shadow <em>casters</em> rather than shadow <em>receivers</em>, so the adaptive sampling is spent on fragments that are usually fully lit. Additionally, valleys can have nearby overhangs just as easily as distant rims, and the height-to-occluder-distance correlation the adaptive scheme assumes does not hold for these visually important cases. We recommend using a fixed &alpha; = 0.5 as the default.</p>

<h2>7. Implementation</h2>

<p>The complete implementation in GLSL. The parallax offset function uses the iterative ray-surface convergence method of Premecz [2006], which scales each step by the height difference and the surface normal&rsquo;s Z component to attenuate on steep slopes. The shadow function uses the square-root distribution (&alpha; = 0.5) with the scale-invariant normalization. Together, the two functions require only the height map (in the alpha channel, with the normal map in RGB) and a small number of iterations.</p>

<pre><code>#define PARALLAX_SCALE 0.04
#define PARALLAX_BIAS -0.02
#define PARALLAX_OFFSET_ITERATIONS 4
#define PARALLAX_SHADOW_ITERATIONS 4

vec2 getParallaxOffset(sampler2D heightSampler, vec2 uv, vec3 eyeDir)
{
    vec3 ray = vec3(0.0);

    for (int i = 0; i &lt; PARALLAX_OFFSET_ITERATIONS; i++)
    {
        vec4 texSample = texture2D(heightSampler, uv + ray.xy);
        float sampledHeight = texSample.a * PARALLAX_SCALE + PARALLAX_BIAS;
        // Convergence driver: steps shrink as ray approaches the surface.
        float heightDiff = sampledHeight - ray.z;
        // Scale step by the surface normal's Z to attenuate on steep slopes.
        ray += eyeDir * heightDiff * texSample.z;
    }

    return ray.xy;
}

float getParallaxShadow(sampler2D heightSampler, vec2 uv, vec3 lightDir)
{
    vec3 step = lightDir * PARALLAX_SCALE;
    float surfaceHeight = texture2D(heightSampler, uv).a;

    float shadow = 0.0;
    for (int i = 1; i &lt;= PARALLAX_SHADOW_ITERATIONS; i++)
    {
        // Square-root distribution: clusters samples in far field,
        // complementing normal-map darkening in the near field
        float t = sqrt(float(i) / float(PARALLAX_SHADOW_ITERATIONS));
        float rayHeight = surfaceHeight + step.z * t;
        float sampleHeight = texture2D(heightSampler, uv + step.xy * t).a;
        // Combined with sqrt sampling, 1/i yields t^{-2} distance attenuation
        // matching solid-angle falloff â€” directional occlusion along the light ray.
        shadow = max(shadow, (sampleHeight - rayHeight) / float(i));
    }

    // Using N as strength makes iteration count a pure quality parameter
    return clamp(1.0 - shadow * float(PARALLAX_SHADOW_ITERATIONS), 0.0, 1.0);
}</code></pre>

<p>The shadow function requires exactly N texture fetches plus one for the surface height, identical to standard hard-shadow POM. The offset function requires one texture fetch per iteration. The additional ALU cost over linear POM is a single sqrt in the shadow function. No additional render targets, temporal accumulation, or stochastic sampling are required.</p>

<h2>8. Limitations and Future Work</h2>

<p>The derivation of &alpha; = 0.5 from solid-angle falloff (Section 6.1) relies on two assumptions. First, the far-field approximation &Omega; &asymp; <em>A</em>/<em>d</em><sup>2</sup>, which assumes the occluder is small relative to its distance from the receiver; for height fields with very large-scale features where this breaks down, other values of &alpha; may produce better results. Second, the directional-occlusion interpretation assumes a lighting environment that includes ambient fill, skylight, or indirect bounce in addition to the primary light source, so that the shadow intensity at a receiver genuinely correlates with the hemispherical solid angle blocked. In scenes lit by a single infinitely distant directional light with no ambient contribution, the solid-angle motivation does not apply&mdash;the visibility along a directional light is a binary event independent of solid angle&mdash;and the technique reduces to a heuristic distance-weighted depth test. In practice, even minimal ambient light is sufficient for the perceptual effect to hold, and the technique remains visually effective in such settings even if the physical justification is weaker. A systematic evaluation across a wider range of height-map types and lighting configurations would clarify the boundaries of applicability.</p>

<p>At N = 2, the technique still produces usable results with subtle banding, but the probe has very limited spatial discrimination. Performance profiling across GPU architectures, particularly regarding texture cache coherence of the non-linear sampling pattern, would clarify the practical cost-quality trade-off.</p>

<p>The technique approximates the <em>opacity</em> component of contact hardening&mdash;nearby occluders produce darker shadows than distant ones, owing to the 1/<em>i</em> weight falloff&mdash;and produces genuinely soft shadow edges through the continuous depth-difference formulation. Because the output is a linear ramp (not a binary threshold), any region where the occluder just barely rises above the ray maps to a partial shadow, creating a gradual spatial transition from lit to shadowed. This softness is structural&mdash;it is a property of the formulation itself and holds regardless of texture filtering mode. However, the edge width is governed by the local height-field gradient and the light angle, not by occluder distance or light source extent. In real penumbrae, shadow edges widen with distance from the occluder; in FXPS, a distant occluder and a nearby occluder with the same height-field gradient profile produce edges of the same width. The technique therefore captures two of the three components of realistic contact hardening (opacity falloff and spatial softness) but not the third (distance-dependent penumbra widening).</p>

<p>Future work could investigate extending the 1/<em>i</em> invariant to non-power-law step functions with appropriately derived weights, or combining FXPS with screen-space techniques that could provide the missing distance-dependent edge widening.</p>

<h2>9. Conclusion</h2>

<p>We have presented Fast Approximate Parallax Shadows (FXPS), a set of improvements to Tatarchuk&rsquo;s [2006] soft POM shadow technique targeting the low-iteration regime. Like the original, FXPS is an empirical heuristic that does not aspire to physical accuracy; it is designed for the bottom of the cost spectrum&mdash;stylized rendering, mobile GPUs, legacy forward pipelines, and any context where visual plausibility at 2&ndash;4 texture fetches matters more than ground-truth visibility.</p>

<p>The improvements are built on two results: first, that generalizing the linear sampling distribution to a power-law t(<em>i</em>) = (<em>i</em>/<em>N</em>)<sup>&alpha;</sup> and combining it with a 1/<em>i</em> weight produces a shadow probe with log-uniform effective sensitivity along the ray, providing a principled basis for the weighting choice; and second, that the correct shadow strength normalization is simply N, making iteration count a pure quality knob independent of all other parameters. We showed that &alpha; = 0.5 (the square-root distribution) is the unique exponent whose distance-attenuation curve matches the solid-angle falloff governing how much of the visible hemisphere an occluder blocks as a function of distance&mdash;the <em>t</em><sup>&minus;2</sup> relationship. Evaluating this hemispherical-occlusion metric along the light vector produces a directional occlusion term: a hybrid between ambient occlusion and hard shadow testing that creates high-frequency, light-aware contrast rather than the uniform cavity darkening of standard AO. This choice is further supported by its synergy with normal-map darkening, avoidance of near-field noise, and dense sampling of the region where real occluders reside.</p>

<p>Against the realistic baseline comparison&mdash;Tatarchuk&rsquo;s original linear soft shadows, hard POM shadows, simple N&middot;L darkening, or no self-shadowing at all&mdash;FXPS offers a meaningful visual improvement for the same texture fetch budget. It requires no precomputed data, no temporal accumulation, and no parameter tuning. For pipelines that need self-shadowing to look good rather than be correct, it is a practical and effective tool.</p>

</body>
</html>
