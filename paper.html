<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Fast Approximate Parallax Shadows</title>
</head>
<body>

<h1>Fast Approximate Parallax Shadows</h1>
<p><em>Improved Soft Occlusion Shadows at Low Sample Counts via Power-Law Sampling</em></p>

<h2>Abstract</h2>

<p>We present Fast Approximate Parallax Shadows (FXPS), a set of improvements to Tatarchuk&rsquo;s [2006] soft POM shadow technique that target the low-iteration regime (N = 2&ndash;4) where linear sampling breaks down. Like the original, FXPS is an empirical heuristic designed for contexts where visual plausibility matters more than physical accuracy&mdash;stylized rendering, mobile GPUs, legacy pipelines, and performance-constrained material layers. It replaces the linear sampling distribution with a power-law function and the accumulation with a 1/<em>i</em> weighted maximum, producing a shadow probe whose effective sensitivity is <em>log-uniform</em> along the ray&mdash;equal attention is allocated to each multiplicative decade of distance. We show that this property is invariant across a single-parameter family of power-law exponents &alpha;, and that the correct shadow strength normalization is simply N (the iteration count), making sample count a pure quality knob.</p>

<p>We analyze the behavior of the power-law family and find that &alpha; = 0.5 (the square-root distribution) produces the best results in practice. At this exponent, the steep t<sup>&minus;2</sup> attenuation curve creates a strong perceptual contact-hardening effect, samples concentrate in the far field where they complement rather than duplicate the normal map&rsquo;s existing near-field darkening, and the ray has risen sufficiently above the surface to avoid spurious self-occlusion from minor surface variations. The result is plausible distance-dependent shadow opacity and spatially soft shadow edges&mdash;arising structurally from the continuous depth-difference formulation&mdash;at 2&ndash;4 iterations per fragment, a meaningful visual improvement over Tatarchuk&rsquo;s linear soft shadows or no self-shadowing at all, for the same texture fetch budget.</p>

<h2>1. Introduction</h2>

<p>Tatarchuk [2006] introduced soft self-shadowing for Parallax Occlusion Mapping (POM) by replacing the binary intersection test of earlier techniques with a continuous depth-difference metric. This was a significant advance: rather than asking whether the height field intersects the light ray (producing hard, banded edges), the technique measures <em>by how much</em> each sample exceeds the ray, producing smooth shadow gradients that look plausible even at moderate sample counts. Her approach uses a linear distribution of samples along the ray, t(<em>i</em>) = <em>i</em>/<em>N</em>, with a distance-based attenuation factor to weight each sample&rsquo;s contribution.</p>

<p>At high iteration counts, this works well. At the very low sample counts demanded by mobile GPUs, legacy pipelines, and performance-constrained material layers (N = 2&ndash;4), however, the linear sampling strategy has two weaknesses. First, it distributes samples uniformly along the ray, wasting budget on regions that may be irrelevant to the local shadow topology. Second, it provides no natural mechanism for contact hardening&mdash;the perceptual property whereby shadows are darkest and sharpest near occluder-receiver contact, becoming lighter and wider with distance.</p>

<p>We present Fast Approximate Parallax Shadows (FXPS), a set of improvements to Tatarchuk&rsquo;s soft POM shadow technique that target the low-iteration regime. Like the original, FXPS is an empirical heuristic&mdash;not a physically based visibility computation&mdash;and is designed for contexts where visual plausibility matters more than physical accuracy. The improvements are:</p>

<p><strong>Power-law sampling with 1/<em>i</em> weighting.</strong> We replace the linear step distribution with a power-law function t(<em>i</em>) = (<em>i</em>/<em>N</em>)<sup>&alpha;</sup> and the accumulation with a weighted maximum using a 1/<em>i</em> weight. We show that this combination produces a shadow probe with <em>log-uniform effective sensitivity</em>&mdash;equal attention to each multiplicative decade of ray distance&mdash;a property that holds across the entire single-parameter family of power-law exponents. We analyze the family and find that &alpha; = 0.5 (the square-root) produces the best results in practice, for reasons we characterize in Section 5.</p>

<p><strong>Scale-invariant normalization.</strong> Setting the shadow strength to N (the iteration count) exactly cancels the dependence of shadow intensity on sample count, making N a pure quality knob with no secondary parameter adjustment required.</p>

<h2>2. Background and Related Work</h2>

<p>Self-shadowing of textured surfaces has been an active area of research for nearly four decades, with techniques spanning a broad spectrum of precomputation cost, runtime overhead, and visual fidelity. We survey the principal approaches to contextualize FXPS within the existing landscape.</p>

<h3>2.1 Horizon Mapping</h3>

<p>The earliest dedicated approach to bump-map self-shadowing is Max&rsquo;s horizon mapping [Max 1988], which precomputes, for each texel and for a discrete set of azimuthal directions, the elevation angle to the horizon&mdash;the highest point visible from that texel along each direction. At runtime, a fragment is shadowed if the light&rsquo;s elevation falls below the stored horizon angle. Sloan and Cohen [2000] mapped the technique to hardware-accelerated rendering, achieving interactive frame rates. Horizon mapping produces hard shadows with no per-frame ray marching, but requires substantial precomputed storage (one horizon angle per direction per texel) and cannot adapt to dynamic height fields. The shadow quality is also limited by the angular discretization of the azimuth directions.</p>

<h3>2.2 Relief Mapping and Linear Ray Marching</h3>

<p>Policarpo, Oliveira, and Comba [2005] introduced relief mapping, which ray-marches through a height field in tangent space using a uniform linear search followed by a binary refinement step. This was the first technique to produce per-pixel self-occlusion and self-shadowing from height maps without precomputation, supporting fully dynamic lighting. McGuire and McGuire [2005] proposed steep parallax mapping, a simplified variant with fewer iterations. Premecz [2006] presented an iterative refinement approach that converges on the ray-surface intersection by scaling each step by the remaining height difference and the surface normal&rsquo;s Z component, providing slope-aware convergence with fewer iterations than linear search. These approaches produce hard shadows: a ray either intersects the height field (shadowed) or does not (lit), with no intermediate softness. Banding artifacts from the discrete linear search are visible at low iteration counts.</p>

<h3>2.3 Parallax Occlusion Mapping with Soft Shadows</h3>

<p>Tatarchuk [2006] extended the relief mapping framework into Parallax Occlusion Mapping (POM), adding a soft shadowing heuristic. The key innovation was replacing the binary intersection test of prior techniques with a continuous depth-difference metric: rather than asking whether the height field intersects the ray (yes or no), the technique measures <em>by how much</em> the height field exceeds the ray at each sample, producing a scalar shadow contribution. This is the fundamental reason soft POM shadows have smooth, gradual edges&mdash;at shadow boundaries, the penetration depth is genuinely small, mapping to partial shadow values through the continuous formulation. This softness is structural and holds regardless of texture filtering mode or sample count. Tatarchuk&rsquo;s technique uses a linear sampling distribution and a distance-based attenuation factor. This is the closest prior work to FXPS and shares the same structural skeleton: a loop over N samples, a depth comparison, and a distance-weighted accumulation. FXPS inherits the soft-edge property and differs in two respects: the use of a power-law (rather than linear) step distribution, and the specific 1/<em>i</em> weight (which we show produces log-uniform sensitivity).</p>

<h3>2.4 Cone Step Mapping</h3>

<p>Dummer [2006] introduced cone step mapping (CSM), which precomputes, for each texel, the radius of the largest empty cone that can be placed at that texel without intersecting the height field. At runtime, the ray advances by the cone radius at each step, enabling aggressive space-leaping that converges in far fewer iterations than linear search. Policarpo and Oliveira [2007] proposed relaxed cone stepping (RCS), which allows the ray to pierce the surface at most once, producing wider cones and faster convergence, followed by a binary refinement. These techniques excel at view-ray intersection finding and can be extended to shadow rays, but require a precomputed cone map (one extra channel per texel), making them unsuitable for dynamic height fields and increasing memory bandwidth.</p>

<h3>2.5 Screen-Space and Hybrid Approaches</h3>

<p>Screen-space contact shadows, widely adopted in modern game engines, ray-march against the depth buffer to detect short-range occlusion that shadow maps miss. These operate at the scene geometry scale and complement shadow maps for macro-scale shadows. Bend Studio&rsquo;s screen-space shadow technique [SIGGRAPH 2023] demonstrated high-quality results for sun shadows in production titles. With the advent of hardware-accelerated ray tracing, hybrid approaches that combine rasterized shadow maps with ray-traced shadows have emerged [AMD FidelityFX Hybrid Shadows]. These operate at the scene level and are orthogonal to FXPS, which addresses self-shadowing within a single height-mapped surface.</p>

<h3>2.6 Radiosity Normal Mapping</h3>

<p>Green [2007] presented efficient self-shadowed radiosity normal mapping at Valve, which precomputes directional occlusion for height-field texels and stores it as a modification to the radiosity normal map basis. This technique is elegant and fast at runtime (requiring no additional per-frame ray marching) but is limited to static height fields and baked lighting configurations.</p>

<h3>2.7 Positioning of FXPS</h3>

<p>FXPS is an empirical heuristic, not a physically based visibility computation. It does not model penumbra geometry, light source extent, or true height-field occlusion. Its soft edges are inherited from the continuous depth-difference formulation (Section 2.3), and its shadow boundaries are approximations that would not match a ground-truth ray tracer at any sample count.</p>

<p>This is by design. FXPS targets the bottom end of the cost spectrum: stylized and mobile rendering, legacy forward pipelines, performance-constrained material layers, and any context where 2&ndash;4 texture fetches is the entire shadow budget. In this regime, the relevant comparison is against Tatarchuk&rsquo;s original linear soft shadows at the same sample count, as well as against simpler alternatives&mdash;hard POM shadows, N&middot;L darkening, or no self-shadowing at all. Against these baselines, FXPS offers a meaningful visual improvement: distance-dependent shadow opacity and spatially soft edges inherited from the continuous depth-difference formulation, all for the same number of texture fetches. It requires no precomputed data structures (unlike horizon mapping, cone step mapping, or radiosity normal mapping), no temporal accumulation, and no additional render targets.</p>

<h2>3. Power-Law Sampling and the 1/<em>i</em> Weight</h2>

<p>Tatarchuk&rsquo;s soft POM shadows use a linear distribution t(<em>i</em>) = <em>i</em>/<em>N</em>, spacing samples uniformly along the ray. We generalize this to a power-law family parameterized by exponent &alpha;:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>t</em>(<em>i</em>) = (<em>i</em> / <em>N</em>)<sup>&alpha;</sup></p>

<p>The exponent &alpha; controls where samples concentrate along the ray. Low &alpha; (e.g. 0.5, the square-root) clusters samples toward the far end, improving detection of distant macro-scale occluders. High &alpha; (e.g. 2.0 or 3.0) clusters samples near the origin, prioritizing contact shadow detection. For N = 4, the sample positions are:</p>

<table border="1" cellpadding="6" cellspacing="0">
<tr><th>&alpha;</th><th>Name</th><th>t&#8321;</th><th>t&#8322;</th><th>t&#8323;</th><th>t&#8324;</th></tr>
<tr><td>0.5</td><td>sqrt</td><td>0.500</td><td>0.707</td><td>0.866</td><td>1.000</td></tr>
<tr><td>1.0</td><td>linear</td><td>0.250</td><td>0.500</td><td>0.750</td><td>1.000</td></tr>
<tr><td>2.0</td><td>quadratic</td><td>0.063</td><td>0.250</td><td>0.563</td><td>1.000</td></tr>
<tr><td>3.0</td><td>cubic</td><td>0.016</td><td>0.125</td><td>0.422</td><td>1.000</td></tr>
</table>
<p><em>Table 1.</em> Sample positions for different power-law exponents at N = 4. Higher &alpha; concentrates samples near the ray origin; lower &alpha; pushes them toward the far end.</p>

<p>Rather than a binary occlusion test, we accumulate the shadow contribution S as a running maximum over a weighted depth difference:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>S</em> = max( <em>S</em>, (<em>H</em>_sample &minus; <em>H</em>_ray) / <em>i</em> )</p>

<p>We now characterize the <em>effective sensitivity</em> of the probe&mdash;the product of sampling density in ray-space and the weight applied at each sample.</p>

<p>For the power-law family, <em>i</em> = <em>N</em> &middot; <em>t</em><sup>1/&alpha;</sup>, giving sampling density d<em>i</em>/d<em>t</em> = (<em>N</em>/&alpha;) &middot; <em>t</em><sup>1/&alpha; &minus; 1</sup>. The weight is <em>w</em> = 1/<em>i</em> = 1/(<em>N</em> &middot; <em>t</em><sup>1/&alpha;</sup>). Their product:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&rho;(<em>t</em>) = (<em>N</em>/&alpha;) &middot; <em>t</em><sup>1/&alpha; &minus; 1</sup> &middot; 1/(<em>N</em> &middot; <em>t</em><sup>1/&alpha;</sup>) = 1/(&alpha;<em>t</em>)</p>

<p>The exponent &alpha; cancels into a constant prefactor. <strong>The 1/<em>i</em> weight produces log-uniform sensitivity for any power-law stepping function.</strong> Reparameterizing in log-space with <em>u</em> = ln(<em>t</em>), the sensitivity per unit <em>u</em> becomes &rho;(<em>t</em>) &middot; <em>t</em> = 1/&alpha;, a constant. The algorithm allocates equal attention to each multiplicative decade of distance&mdash;the interval [0.01, 0.1] receives the same total sensitivity as [0.1, 1.0].</p>

<canvas id="fig1" width="700" height="380"></canvas>
<p><em>Figure 1.</em> Left: effective sensitivity &rho;(t) = 1/(&alpha;t) for different power-law exponents. All curves are the same 1/t shape, scaled by 1/&alpha;. Right: sensitivity per log-decade &rho;(t)&middot;t = 1/&alpha;, a constant for each &alpha;. This is the log-uniform invariant.</p>

<h3>3.1 Distance-Attenuation Interpretation</h3>

<p>While the log-uniform invariant holds for any &alpha;, the 1/<em>i</em> weight maps to a <em>different distance-attenuation curve</em> depending on the exponent. Since <em>i</em> = <em>N</em> &middot; <em>t</em><sup>1/&alpha;</sup>, the weight expressed in ray-space is:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;1/<em>i</em> = (1/<em>N</em>) &middot; <em>t</em><sup>&minus;1/&alpha;</sup></p>

<table border="1" cellpadding="6" cellspacing="0">
<tr><th>&alpha;</th><th>Step</th><th>Ray-space atten.</th><th>Falloff</th><th>Character</th></tr>
<tr><td>0.5</td><td>sqrt</td><td>t<sup>&minus;2</sup></td><td>Inverse square</td><td>Aggressive</td></tr>
<tr><td>1.0</td><td>linear</td><td>t<sup>&minus;1</sup></td><td>Inverse linear</td><td>Moderate</td></tr>
<tr><td>2.0</td><td>quadratic</td><td>t<sup>&minus;0.5</sup></td><td>Inverse sqrt</td><td>Gentle</td></tr>
<tr><td>3.0</td><td>cubic</td><td>t<sup>&minus;0.33</sup></td><td>Near-constant</td><td>Very gentle</td></tr>
</table>
<p><em>Table 2.</em> Distance-attenuation curves implied by the 1/i weight at different power-law exponents. As &alpha; increases, the attenuation in ray-space becomes softer.</p>

<canvas id="fig2" width="700" height="420"></canvas>
<p><em>Figure 2.</em> The same 1/i weight projected into ray-space at different &alpha;. At &alpha; = 0.5 (blue), the curve is steep&mdash;near-field positions receive far more weight than far-field. At &alpha; = 2.0 (amber), the curve is nearly flat. All curves converge to 1 at t = 1.</p>

<canvas id="fig3" width="700" height="600"></canvas>
<p><em>Figure 3.</em> Discrete sample placement and weights for N = 4 at each &alpha;. Bar height shows the 1/i weight; bar position shows where that sample falls on the ray. The red shaded region is unsampled. As &alpha; increases, the first sample moves toward the ray origin and the unsampled gap shrinks.</p>

<p>As &alpha; increases, the attenuation in ray-space becomes softer. It is important to understand what this means in the discrete case. The 1/<em>i</em> weight assigns the same values regardless of &alpha;: {1, 1/2, 1/3, 1/4} for i = {1, 2, 3, 4}. It is a fixed index-space ramp that does not know or care about the sampling distribution. What &alpha; controls is <em>where those indices land on the ray</em>.</p>

<p>For &alpha; = 0.5, the four samples land at t = {0.50, 0.71, 0.87, 1.00}. Weight 1 is assigned to t = 0.50, weight 0.25 to t = 1.00&mdash;a 4:1 ratio across a 2:1 distance range. Projected into ray-space, this is a steep falloff. The first sample gets strongly prioritized over the last, producing a perceptual effect that <em>partially</em> resembles contact hardening across the [0.5, 1.0] band&mdash;an opacity gradient where nearby occluders cast darker shadows than distant ones. Note that this is not detecting true contact shadows&mdash;the entire [0, 0.5) range is unsampled, and no amount of weight amplification can detect an occluder at a position that was never queried.</p>

<p>For &alpha; = 2.0, the four samples land at t = {0.063, 0.25, 0.56, 1.00}. The same weight 1 is now assigned to t = 0.063, genuinely in the near field. Weight 0.25 goes to t = 1.00&mdash;a 4:1 ratio across a 16:1 distance range. Projected into ray-space, this is a much gentler curve. The first sample does not overwhelmingly dominate; the far-field samples retain meaningful influence. This is appropriate because the samples already span nearly the full ray, so the weight does not need to impose a strong spatial preference.</p>

<p>The different distance-attenuation curves are therefore not a compensatory mechanism&mdash;the weight is not &ldquo;aware&rdquo; of the sampling distribution and adapting to it. Rather, the same fixed index-space ramp is <em>projected through a different nonlinear mapping</em> depending on &alpha;. When that mapping compresses a wide ray-distance range into a narrow index range (low &alpha;), the projected curve is steep. When it stretches a wide ray-distance range across many indices (high &alpha;), the projected curve is gentle. The log-uniform effective sensitivity falls out of the <em>algebra</em> of this projection, not from any adaptive behavior in the weight itself.</p>

<p>The 1/<em>i</em> weight is the unique index-space ramp that, when projected through any power-law step function, yields log-uniform sensitivity in the continuous limit. It is a principled choice of weight&mdash;but it is a fixed formula applied uniformly, and the apparent &ldquo;adaptation&rdquo; to different &alpha; regimes is entirely a consequence of the projection geometry.</p>

<h3>3.2 Soft Shadow Edges</h3>

<p>Both FXPS and Tatarchuk&rsquo;s original technique produce spatially soft shadow edges, owing to the continuous depth-difference formulation they share (Section 2.3). The output clamp(1 &minus; <em>N</em> &middot; <em>S</em>, 0, 1) is a linear ramp, not a step function. At a shadow boundary, there is always a spatial band of fragments where the occluder just barely rises above the ray&mdash;a small positive depth difference that maps to a partial shadow value. This softness is structural: it is a property of the formulation itself and holds regardless of texture filtering mode. The width of the soft transition zone depends on the height-field gradient and the light angle, but not on occluder distance or light source extent&mdash;a distinction from true penumbrae explored further in Section 8.</p>

<h3>3.3 The Continuous-Discrete Gap</h3>

<p>An important caveat: the log-uniform property is a statement about the continuous limit. At very low iteration counts, the discrete sample placement dominates behavior. With &alpha; = 0.5 and N = 4, the first sample falls at t = 0.50&mdash;the entire [0, 0.5) range is unsampled. The four samples have weights 1, 1/2, 1/3, 1/4, forming a monotonically decreasing discrete ramp over the [0.5, 1.0] band. The visual result at low N is driven as much by this discrete weight gradient as by the continuous log-uniform property.</p>

<p>The continuous analysis nevertheless justifies the <em>choice</em> of 1/<em>i</em> as the weighting function: it is the unique weight that produces log-uniform sensitivity in the limit, and it provides a principled starting point from which the discrete behavior inherits its favorable gradient. The question becomes: which value of &alpha; makes the best use of these limited discrete samples? Section 5 addresses this question.</p>

<h2>4. Scale-Invariant Normalization</h2>

<p>For an occluder at fixed position <em>t</em>* along the ray, the nearest sample index scales as <em>i</em>* &asymp; <em>N</em> &middot; <em>t</em>*<sup>1/&alpha;</sup>. The raw shadow contribution is:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&Delta;<em>H</em> / <em>i</em>* &asymp; &Delta;<em>H</em> / (<em>N</em> &middot; <em>t</em>*<sup>1/&alpha;</sup>)</p>

<p>This is inversely proportional to N. Multiplying by strength = N cancels the dependence exactly:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>S</em>_final = <em>N</em> &middot; &Delta;<em>H</em> / (<em>N</em> &middot; <em>t</em>*<sup>1/&alpha;</sup>) = &Delta;<em>H</em> / <em>t</em>*<sup>1/&alpha;</sup></p>

<p>The shadow response becomes <strong>independent of iteration count</strong>. N can be adjusted purely as a quality knob&mdash;no secondary strength parameter is needed. Shadow intensity remains consistent across quality levels; only the spatial resolution of occluder detection changes.</p>

<p>This also explains a practical observation: setting the shadow strength above N causes banding at low sample counts. The shadow value saturates more aggressively, destroying the smooth gradient between fully lit and fully shadowed regions. With strength = N, the output stays in the linear range of the clamp, preserving the continuous falloff.</p>

<p>Crucially, this result holds for <em>any</em> &alpha; and <em>any</em> N. It is an exact discrete property, not an asymptotic one, which makes it robust in the low-iteration regime where the technique operates.</p>

<h2>5. Choosing the Exponent</h2>

<p>The analysis so far establishes a family of shadow probes parameterized by &alpha;, all sharing the log-uniform invariant and the strength = N normalization. The question remains: what value of &alpha; should be used?</p>

<h3>5.1 The Case for &alpha; = 0.5</h3>

<p>Empirically, &alpha; = 0.5 (the square-root distribution) consistently produces the best visual results across a range of height maps and lighting conditions. We identify four reasons for this.</p>

<p><strong>Synergy with normal mapping.</strong> In any pipeline that uses normal maps alongside height maps, the perturbed Lambertian term &langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; already darkens fragments whose surface normal faces away from the light. This provides effective near-field self-shadowing for free: a small bump or ridge adjacent to the shading point darkens it through normal-map orientation, producing the same visual effect that a near-field shadow sample would detect. High &alpha; concentrates samples in exactly this near-field region, duplicating work that the normal map already handles. &alpha; = 0.5 avoids this redundancy by placing all samples in the far field (t &ge; 0.5 at N = 4), where it detects occluders that no amount of normal-map darkening can capture&mdash;a distant rim or ridge blocking light to a surface patch that locally faces the light.</p>

<p><strong>Near-field noise avoidance.</strong> The shadow ray starts at the surface height and rises along the light direction. At small t, the ray is barely above the surface&mdash;the geometric separation between ray and height field is minimal. In this region, minor surface variations that are not meaningful occluders can produce small positive depth differences, registering as spurious shadow contributions. High &alpha; pushes the first sample into this noisy zone (&alpha; = 2.0 places it at t = 0.063, where the ray has risen by only 6.3% of its total height excursion). &alpha; = 0.5 keeps all samples at t &ge; 0.50, where the ray has risen enough that only genuinely taller features register as occluders.</p>

<p><strong>Steep weight gradient.</strong> At &alpha; = 0.5, the 1/<em>i</em> weight projected into ray-space produces a t<sup>&minus;2</sup> attenuation curve&mdash;the steepest in the power-law family (Table 2, Figure 2). Even though all four samples live in the [0.5, 1.0] band, the first sample receives four times the weight of the last. Across the surface, this creates a strong spatial opacity gradient: occluders detected at t &asymp; 0.5 cast substantially darker shadows than those at t &asymp; 1.0. The perceptual result closely resembles contact hardening, despite the samples never reaching the true near field.</p>

<p><strong>Dense sampling where occluders reside.</strong> At typical POM scales (PARALLAX_SCALE &asymp; 0.04), the entire shadow ray is physically short. Most real shadow-casting geometry&mdash;brick edges, ridge lines, surface ledges&mdash;creates occlusion at intermediate distances along the ray, not at the extreme near field. Four samples in [0.5, 1.0] provide dense, well-separated coverage of this zone. Higher &alpha; spreads the same four samples across [0.06, 1.0], giving sparse coverage everywhere and dense coverage nowhere.</p>

<h3>5.2 Note on Height-Adaptive &alpha;</h3>

<p>We also explored a height-adaptive variant in which the exponent varies per-fragment: &alpha; = mix(&alpha;_min, &alpha;_max, <em>h</em>), where <em>h</em> is the surface height. The geometric motivation was that peaks face nearby occluders (calling for high &alpha;) while valleys face distant ones (calling for low &alpha;). While theoretically appealing, this approach did not improve results in practice. We believe this is because the fragments that receive the most visually important shadows are at mid-to-low elevation&mdash;below the features that cast shadows on them&mdash;and these fragments are precisely the ones where the adaptive scheme assigns low &alpha;, which is already the best choice regardless. Meanwhile, peak fragments receive high &alpha; but are typically shadow <em>casters</em> rather than shadow <em>receivers</em>, so the adaptive sampling is spent on fragments that are usually fully lit. Additionally, valleys can have nearby overhangs just as easily as distant rims, and the height-to-occluder-distance correlation the adaptive scheme assumes does not hold for these visually important cases. We recommend using a fixed &alpha; = 0.5 as the default.</p>

<h2>6. Notes on PBR Integration</h2>

<p>FXPS is designed as an empirical technique for stylized and performance-constrained rendering, where it can serve as a simple multiplicative darkening factor applied to the final lighting. However, it can also be inserted into a physically based pipeline if desired. This section provides guidance for that use case, characterizing how FXPS interacts with the self-shadowing mechanisms already present in a standard PBR lighting equation.</p>

<p>In a standard PBR direct lighting pipeline, the outgoing radiance at a surface point <strong>x</strong> with view direction <strong>v</strong> and light direction <strong>l</strong> is:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>L</em>_o(<strong>x</strong>, <strong>v</strong>) = <em>f</em>_r(<strong>l</strong>, <strong>v</strong>, <strong>n&prime;</strong>) &middot; <em>L</em>_i &middot; &langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; &middot; <em>V</em>(<strong>x</strong>, <strong>l</strong>)</p>

<p>where <em>f</em>_r is the BRDF, <em>L</em>_i is the incident radiance from the light, <strong>n&prime;</strong> is the perturbed (normal-mapped) surface normal, and <em>V</em> is a geometric visibility term. If FXPS is used within such a pipeline, it provides a replacement for <em>V</em>. The following subsections characterize how this interacts with the other self-shadowing mechanisms in the equation.</p>

<h3>6.1 Multi-Scale Shadow Decomposition</h3>

<p>A height-mapped PBR surface contains three distinct spatial scales of self-occlusion, each handled by a different term in the rendering equation:</p>

<p><strong>Micro-scale: the Smith shadowing-masking function G(l, v, m).</strong> Inside the Cook-Torrance specular BRDF, the geometry term <em>G</em> models the statistical self-occlusion of sub-texel microfacets as a function of roughness and the half-vector. It operates at the scale of surface microgeometry (micrometers)&mdash;far below the resolution of any height map. The specular BRDF is typically written as:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>f</em>_spec = <em>D</em> &middot; <em>F</em> &middot; <em>G</em> / (4 &langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; &langle;<strong>n&prime;</strong> &middot; <strong>v</strong>&rangle;)</p>

<p>where <em>D</em> is the microfacet distribution (e.g. GGX), <em>F</em> is the Fresnel term, and <em>G</em> encodes the proportion of microfacets that are both visible to the camera and not shadowed from the light. This is a <em>statistical</em> self-shadowing model&mdash;it does not resolve individual occluders but rather models their aggregate effect on the lobe.</p>

<p><strong>Meso-scale: the perturbed Lambertian term &langle;n&prime; &middot; l&rangle;.</strong> The clamped dot product between the normal-mapped normal <strong>n&prime;</strong> and the light direction <strong>l</strong> captures orientation-dependent darkening from surface detail encoded in the normal map. While not geometric shadowing in the strict sense, it approximates the visual effect of self-occlusion at the normal-map frequency (typically millimeter-scale surface undulations). Features that face away from the light are darkened; features facing the light receive full irradiance.</p>

<p><strong>Macro-scale: the visibility term V(x, l).</strong> This is the geometric visibility between the shading point and the light, evaluated along the height field. In standard POM, this term is either absent (no self-shadowing), binary (hard shadow), or smoothed via multi-sample averaging. FXPS replaces it with a continuous, power-law weighted probe:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>V</em>_FXPS(<strong>x</strong>, <strong>l</strong>) = clamp(1 &minus; <em>N</em> &middot; max<sub><em>i</em></sub> (<em>H</em>_sample(<em>t</em>_<em>i</em>) &minus; <em>H</em>_ray(<em>t</em>_<em>i</em>)) / <em>i</em>, 0, 1)</p>

<p>This term operates at the height-map texel scale (millimeters to centimeters)&mdash;the same frequency as the normal map, but capturing a fundamentally different phenomenon: whether a distant feature <em>occludes</em> the light, rather than whether the local surface <em>faces</em> it.</p>

<h3>6.2 The Complete Reflectance Equation</h3>

<p>Substituting the multi-scale terms, the full direct lighting equation for a single light becomes:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<em>L</em>_o = [<em>f</em>_spec + <em>f</em>_diff] &middot; <em>L</em>_i &middot; &langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; &middot; <em>V</em>_FXPS</p>

<p>where <em>f</em>_spec contains <em>G</em> internally and <em>f</em>_diff is the diffuse BRDF (e.g. Lambertian or Oren-Nayar). The three self-shadowing mechanisms enter multiplicatively: <em>G</em> modulates the specular lobe, &langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; modulates the total irradiance via surface orientation, and <em>V</em>_FXPS modulates the incoming radiance via macro-scale geometric occlusion.</p>

<p>The multiplicative decomposition is physically consistent because <em>V</em>_FXPS modulates <em>incoming radiance before it reaches the BRDF</em>. It is not a modification of the reflectance function itself, but of the light transport. The BRDF&rsquo;s internal <em>G</em> term then further attenuates based on microfacet statistics, operating on the already-modulated irradiance. This is consistent with the rendering equation&rsquo;s structure, where visibility and reflectance are separable.</p>

<h3>6.3 Scale Separation and Double-Counting</h3>

<p>The multiplicative composition of <em>G</em>, &langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle;, and <em>V</em>_FXPS is physically justified when they operate at non-overlapping spatial frequencies. In practice, this assumption holds well:</p>

<p>The Smith <em>G</em> function models sub-texel microgeometry at the scale of the roughness parameter (typically micrometers). POM height maps encode features at texel resolution (millimeters to centimeters). The frequency gap between these scales is typically three or more orders of magnitude, making overlap negligible.</p>

<p>The meso-scale &langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; term and the macro-scale <em>V</em>_FXPS term both operate at roughly the same spatial frequency (the height-map/normal-map texel), but they capture <em>orthogonal phenomena</em>. The dot product measures local surface orientation; <em>V</em>_FXPS measures non-local occlusion along the light ray. A surface patch can face the light (&langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; &asymp; 1) yet still be occluded by a distant ridge (<em>V</em>_FXPS &lt; 1), or face away from the light (&langle;<strong>n&prime;</strong> &middot; <strong>l</strong>&rangle; &asymp; 0) while being geometrically unoccluded (<em>V</em>_FXPS = 1). Their product correctly captures both effects simultaneously.</p>

<p>The one scenario where mild double-counting could arise is if the height map contains features at a scale comparable to the microfacet roughness. In such a case, both <em>G</em> and <em>V</em>_FXPS would attenuate for the same geometric cause. In practice, this is extremely unlikely in real-time rendering pipelines: height maps are authored at a resolution of hundreds of texels per meter, while roughness models sub-micron geometry. Nonetheless, artists should be aware that extremely rough materials (roughness &gt; 0.9) combined with very high-frequency height maps could in principle produce slightly over-darkened results.</p>

<h3>6.4 Energy Conservation</h3>

<p><em>V</em>_FXPS is bounded to [0, 1] and is purely attenuative&mdash;it can reduce incoming radiance but never amplify it. This guarantees that the technique cannot violate energy conservation. The total outgoing radiance at any fragment is always less than or equal to the radiance that would be computed without the visibility term. In the absence of any occluders, <em>V</em>_FXPS evaluates to exactly 1.0, reducing to the unshadowed case.</p>

<p>Note that <em>V</em>_FXPS does not account for indirect illumination or interreflection within the height field. Light blocked by the visibility term is simply removed, not redistributed. In pipelines that include screen-space or probe-based global illumination, the ambient or indirect term should <em>not</em> be modulated by <em>V</em>_FXPS, as the indirect lighting has already been attenuated through its own visibility evaluation. The FXPS visibility term should be applied only to direct light contributions.</p>

<h2>7. Implementation</h2>

<p>The complete implementation in GLSL. The parallax offset function uses the iterative ray-surface convergence method of Premecz [2006], which scales each step by the height difference and the surface normal&rsquo;s Z component to attenuate on steep slopes. The shadow function uses the square-root distribution (&alpha; = 0.5) with the scale-invariant normalization. Together, the two functions require only the height map (in the alpha channel, with the normal map in RGB) and a small number of iterations.</p>

<pre><code>#define PARALLAX_SCALE 0.04
#define PARALLAX_BIAS -0.02
#define PARALLAX_OFFSET_ITERATIONS 4
#define PARALLAX_SHADOW_ITERATIONS 4

vec2 getParallaxOffset(sampler2D heightSampler, vec2 uv, vec3 eyeDir)
{
    vec3 ray = vec3(0.0);

    for (int i = 0; i &lt; PARALLAX_OFFSET_ITERATIONS; i++)
    {
        vec4 texSample = texture2D(heightSampler, uv + ray.xy);
        float sampledHeight = texSample.a * PARALLAX_SCALE + PARALLAX_BIAS;
        // Convergence driver: steps shrink as ray approaches the surface.
        float heightDiff = sampledHeight - ray.z;
        // Scale step by the surface normal's Z to attenuate on steep slopes.
        ray += eyeDir * heightDiff * texSample.z;
    }

    return ray.xy;
}

float getParallaxShadow(sampler2D heightSampler, vec2 uv, vec3 lightDir)
{
    vec3 step = lightDir * PARALLAX_SCALE;
    float surfaceHeight = texture2D(heightSampler, uv).a;

    float shadow = 0.0;
    for (int i = 1; i &lt;= PARALLAX_SHADOW_ITERATIONS; i++)
    {
        // Square-root distribution: clusters samples in far field,
        // complementing normal-map darkening in the near field
        float t = sqrt(float(i) / float(PARALLAX_SHADOW_ITERATIONS));
        float rayHeight = surfaceHeight + step.z * t;
        float sampleHeight = texture2D(heightSampler, uv + step.xy * t).a;
        // 1/i weighting yields log-uniform sensitivity across the power-law family,
        // allocating equal attention to each multiplicative decade of ray distance
        shadow = max(shadow, (sampleHeight - rayHeight) / float(i));
    }

    // Using N as strength makes iteration count a pure quality parameter
    return clamp(1.0 - shadow * float(PARALLAX_SHADOW_ITERATIONS), 0.0, 1.0);
}</code></pre>

<p>The shadow function requires exactly N texture fetches plus one for the surface height, identical to standard hard-shadow POM. The offset function requires one texture fetch per iteration. The additional ALU cost over linear POM is a single sqrt in the shadow function. No additional render targets, temporal accumulation, or stochastic sampling are required.</p>

<h2>8. Limitations and Future Work</h2>

<p>The recommendation of &alpha; = 0.5 is empirically motivated rather than theoretically derived. While the arguments in Section 5.1 provide plausible explanations for its effectiveness, a systematic evaluation across a wide range of height-map types, lighting conditions, and artistic styles would strengthen the recommendation. Other values of &alpha; may be preferable for specific use cases&mdash;for example, height maps with unusually low-frequency content might benefit from higher &alpha; to detect nearby occluders that the normal map does not capture.</p>

<p>At N = 2, the technique still produces usable results with subtle banding, but the probe has very limited spatial discrimination. Performance profiling across GPU architectures, particularly regarding texture cache coherence of the non-linear sampling pattern, would clarify the practical cost-quality trade-off.</p>

<p>The technique approximates the <em>opacity</em> component of contact hardening&mdash;nearby occluders produce darker shadows than distant ones, owing to the 1/<em>i</em> weight falloff&mdash;and produces genuinely soft shadow edges through the continuous depth-difference formulation. Because the output is a linear ramp (not a binary threshold), any region where the occluder just barely rises above the ray maps to a partial shadow, creating a gradual spatial transition from lit to shadowed. This softness is structural&mdash;it is a property of the formulation itself and holds regardless of texture filtering mode. However, the edge width is governed by the local height-field gradient and the light angle, not by occluder distance or light source extent. In real penumbrae, shadow edges widen with distance from the occluder; in FXPS, a distant occluder and a nearby occluder with the same height-field gradient profile produce edges of the same width. The technique therefore captures two of the three components of realistic contact hardening (opacity falloff and spatial softness) but not the third (distance-dependent penumbra widening).</p>

<p>Future work could investigate extending the 1/<em>i</em> invariant to non-power-law step functions with appropriately derived weights, or combining FXPS with screen-space techniques that could provide the missing distance-dependent edge widening.</p>

<h2>9. Conclusion</h2>

<p>We have presented Fast Approximate Parallax Shadows (FXPS), a set of improvements to Tatarchuk&rsquo;s [2006] soft POM shadow technique targeting the low-iteration regime. Like the original, FXPS is an empirical heuristic that does not aspire to physical accuracy; it is designed for the bottom of the cost spectrum&mdash;stylized rendering, mobile GPUs, legacy forward pipelines, and any context where visual plausibility at 2&ndash;4 texture fetches matters more than ground-truth visibility.</p>

<p>The improvements are built on two results: first, that generalizing the linear sampling distribution to a power-law t(<em>i</em>) = (<em>i</em>/<em>N</em>)<sup>&alpha;</sup> and combining it with a 1/<em>i</em> weight produces a shadow probe with log-uniform effective sensitivity along the ray, providing a principled basis for the weighting choice; and second, that the correct shadow strength normalization is simply N, making iteration count a pure quality knob independent of all other parameters. We analyzed the behavior of the power-law family and found that &alpha; = 0.5 (the square-root distribution) produces the best results in practice, owing to its synergy with normal-map darkening, avoidance of near-field noise, steep perceptual weight gradient, and dense sampling of the region where real occluders reside.</p>

<p>Against the realistic baseline comparison&mdash;Tatarchuk&rsquo;s original linear soft shadows, hard POM shadows, simple N&middot;L darkening, or no self-shadowing at all&mdash;FXPS offers a meaningful visual improvement for the same texture fetch budget. It requires no precomputed data, no temporal accumulation, and no parameter tuning. For pipelines that need self-shadowing to look good rather than be correct, it is a practical and effective tool.</p>

<script>
// ─── shared helpers ──────────────────────────────────────────────────────────

const COLORS = {
  a05: '#4878cf',   // blue
  a10: '#3aaa35',   // green
  a20: '#e08020',   // amber
  a30: '#d63030',   // red
};

function drawAxes(ctx, lx, rx, by, ty, xmin, xmax, ymin, ymax,
                  xticks, yticks, xlabel, ylabel, title) {
  ctx.save();
  ctx.strokeStyle = '#000';
  ctx.lineWidth = 1;

  // border
  ctx.strokeRect(lx, ty, rx - lx, by - ty);

  const xs = t => lx + (t - xmin) / (xmax - xmin) * (rx - lx);
  const ys = v => by - (v - ymin) / (ymax - ymin) * (by - ty);

  ctx.fillStyle = '#000';
  ctx.font = '11px sans-serif';
  ctx.textAlign = 'center';

  // x ticks
  for (const v of xticks) {
    const x = xs(v);
    ctx.beginPath(); ctx.moveTo(x, by); ctx.lineTo(x, by + 5); ctx.stroke();
    ctx.fillText(v.toFixed(v === Math.round(v) ? 2 : 2), x, by + 16);
    // grid
    ctx.save(); ctx.strokeStyle = '#ddd'; ctx.lineWidth = 0.5;
    ctx.beginPath(); ctx.moveTo(x, ty); ctx.lineTo(x, by); ctx.stroke();
    ctx.restore();
  }
  // x label
  ctx.fillText(xlabel, (lx + rx) / 2, by + 30);

  // y ticks
  ctx.textAlign = 'right';
  for (const v of yticks) {
    const y = ys(v);
    ctx.beginPath(); ctx.moveTo(lx, y); ctx.lineTo(lx - 5, y); ctx.stroke();
    ctx.fillText(v % 1 === 0 ? v.toFixed(1) : v.toFixed(1), lx - 8, y + 4);
    ctx.save(); ctx.strokeStyle = '#ddd'; ctx.lineWidth = 0.5;
    ctx.beginPath(); ctx.moveTo(lx, y); ctx.lineTo(rx, y); ctx.stroke();
    ctx.restore();
  }

  // y label (rotated)
  ctx.save();
  ctx.translate(lx - 40, (ty + by) / 2);
  ctx.rotate(-Math.PI / 2);
  ctx.textAlign = 'center';
  ctx.fillText(ylabel, 0, 0);
  ctx.restore();

  // title
  ctx.textAlign = 'center';
  ctx.font = 'bold 12px sans-serif';
  ctx.fillText(title, (lx + rx) / 2, ty - 8);
  ctx.restore();

  return { xs, ys };
}

function plotCurve(ctx, xs, ys, fn, xmin, xmax, steps, color) {
  ctx.save();
  ctx.strokeStyle = color;
  ctx.lineWidth = 2;
  ctx.beginPath();
  let first = true;
  for (let k = 0; k <= steps; k++) {
    const t = xmin + (xmax - xmin) * k / steps;
    const v = fn(t);
    if (!isFinite(v)) { first = true; continue; }
    if (first) { ctx.moveTo(xs(t), ys(v)); first = false; }
    else ctx.lineTo(xs(t), ys(v));
  }
  ctx.stroke();
  ctx.restore();
}

function legend(ctx, x, y, items, fontSize) {
  ctx.save();
  ctx.font = (fontSize || 11) + 'px sans-serif';
  ctx.strokeStyle = '#ccc';
  ctx.lineWidth = 0.5;
  const lw = 20, pad = 6, lineH = 16;
  const w = 155, h = items.length * lineH + pad * 2;
  ctx.fillStyle = '#fff';
  ctx.fillRect(x, y, w, h);
  ctx.strokeRect(x, y, w, h);
  items.forEach(([color, label], i) => {
    const ly = y + pad + lineH * i + lineH / 2;
    ctx.strokeStyle = color; ctx.lineWidth = 2;
    ctx.beginPath(); ctx.moveTo(x + pad, ly); ctx.lineTo(x + pad + lw, ly); ctx.stroke();
    ctx.fillStyle = '#000'; ctx.lineWidth = 1;
    ctx.textAlign = 'left';
    ctx.fillText(label, x + pad + lw + 5, ly + 4);
  });
  ctx.restore();
}

// ─── Figure 1 ─────────────────────────────────────────────────────────────────
(function() {
  const canvas = document.getElementById('fig1');
  const ctx = canvas.getContext('2d');
  const W = canvas.width, H = canvas.height;

  ctx.fillStyle = '#fff';
  ctx.fillRect(0, 0, W, H);

  // main title
  ctx.fillStyle = '#000';
  ctx.font = 'bold 13px sans-serif';
  ctx.textAlign = 'center';

  const half = W / 2;

  // LEFT PLOT: rho(t) = 1/(alpha*t)
  const lx1 = 55, rx1 = half - 20;
  const lx2 = half + 40, rx2 = W - 20;
  const by = H - 40, ty = 30;
  const xticks = [0.00, 0.25, 0.50, 0.75, 1.00];
  const yticks1 = [0, 2, 4, 6, 8, 10, 12, 14];
  const yticks2 = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0];

  const {xs: xs1, ys: ys1} = drawAxes(ctx, lx1, rx1, by, ty,
    0, 1, 0, 15, xticks, yticks1,
    'Ray distance  t', 'ρ(t) = 1/(αt)', 'Effective Sensitivity');

  const {xs: xs2, ys: ys2} = drawAxes(ctx, lx2, rx2, by, ty,
    0, 1, 0, 3.0, xticks, yticks2,
    'Ray distance  t', 'ρ(t)·t = 1/α', 'Sensitivity per Log-Decade');

  const alphas = [0.5, 1.0, 2.0, 3.0];
  const cols = [COLORS.a05, COLORS.a10, COLORS.a20, COLORS.a30];

  alphas.forEach((a, i) => {
    // clip left plot to ymax=15
    ctx.save();
    ctx.rect(lx1, ty, rx1 - lx1, by - ty);
    ctx.clip();
    plotCurve(ctx, xs1, ys1, t => 1 / (a * t), 0.001, 1, 400, cols[i]);
    ctx.restore();

    // right plot: flat lines at 1/alpha
    ctx.save();
    ctx.strokeStyle = cols[i]; ctx.lineWidth = 2;
    ctx.beginPath();
    ctx.moveTo(xs2(0), ys2(1/a));
    ctx.lineTo(xs2(1), ys2(1/a));
    ctx.stroke();
    ctx.restore();
  });

  legend(ctx, lx1 + 5, ty + 5, [
    [COLORS.a05, 'α = 0.5 (sqrt)'],
    [COLORS.a10, 'α = 1.0 (linear)'],
    [COLORS.a20, 'α = 2.0 (quadratic)'],
    [COLORS.a30, 'α = 3.0 (cubic)'],
  ]);

  legend(ctx, lx2 + 5, ty + 5, [
    [COLORS.a05, '1/α = 2.0'],
    [COLORS.a10, '1/α = 1.0'],
    [COLORS.a20, '1/α = 0.5'],
    [COLORS.a30, '1/α = 0.3'],
  ]);
})();

// ─── Figure 2 ─────────────────────────────────────────────────────────────────
(function() {
  const canvas = document.getElementById('fig2');
  const ctx = canvas.getContext('2d');
  const W = canvas.width, H = canvas.height;

  ctx.fillStyle = '#fff';
  ctx.fillRect(0, 0, W, H);

  const lx = 65, rx = W - 20, by = H - 40, ty = 35;
  const xticks = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0];
  const yticks = [0, 2.5, 5.0, 7.5, 10.0, 12.5, 15.0, 17.5, 20.0];

  const {xs, ys} = drawAxes(ctx, lx, rx, by, ty,
    0, 1, 0, 20,
    xticks, yticks,
    'Ray distance  t',
    'Relative weight  w(t) / w(1)',
    'Distance-Attenuation Curves in Ray-Space');

  const alphas = [0.5, 1.0, 2.0, 3.0];
  const cols = [COLORS.a05, COLORS.a10, COLORS.a20, COLORS.a30];

  alphas.forEach((a, i) => {
    // w(t) = t^(-1/a), w(1) = 1, so ratio = t^(-1/a)
    ctx.save();
    ctx.rect(lx, ty, rx - lx, by - ty);
    ctx.clip();
    plotCurve(ctx, xs, ys, t => Math.pow(t, -1/a), 0.002, 1, 500, cols[i]);
    ctx.restore();
  });

  // annotations
  ctx.save();
  ctx.fillStyle = COLORS.a05;
  ctx.font = 'italic 11px sans-serif';
  ctx.fillText('Steep falloff', xs(0.08) + 65, ys(16));
  ctx.fillText('(aggressive)', xs(0.08) + 65, ys(16) + 14);
  ctx.fillStyle = COLORS.a20;
  ctx.fillText('Gentle falloff', xs(0.07) + 5, ys(5.5));
  ctx.restore();

  legend(ctx, rx - 165, ty + 5, [
    [COLORS.a05, 'α = 0.5 (sqrt)'],
    [COLORS.a10, 'α = 1.0 (linear)'],
    [COLORS.a20, 'α = 2.0 (quadratic)'],
    [COLORS.a30, 'α = 3.0 (cubic)'],
  ]);
})();

// ─── Figure 3 ─────────────────────────────────────────────────────────────────
(function() {
  const canvas = document.getElementById('fig3');
  const ctx = canvas.getContext('2d');
  const W = canvas.width, H = canvas.height;
  ctx.fillStyle = '#fff'; ctx.fillRect(0, 0, W, H);

  const N = 4;
  const alphas = [0.5, 1.0, 2.0, 3.0];
  const cols   = [COLORS.a05, COLORS.a10, COLORS.a20, COLORS.a30];
  const names  = ['α = 0.5 (sqrt)', 'α = 1.0 (linear)', 'α = 2.0 (quadratic)', 'α = 3.0 (cubic)'];

  const lx = 55, rx = W - 15;
  const totalH = H - 10;
  const panelH = totalH / 4;
  const marginTop = 20, marginBot = 28;
  const xticks = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0];

  ctx.fillStyle = '#000';
  ctx.font = 'bold 13px sans-serif';
  ctx.textAlign = 'center';
  ctx.fillText('Sample Placement and Weights at N = 4', W / 2, 14);

  alphas.forEach((a, pi) => {
    const panelY = 18 + pi * panelH;
    const ty = panelY + marginTop;
    const by = panelY + panelH - marginBot;

    const ymin = 0, ymax = 1.2;
    const {xs, ys} = drawAxes(ctx, lx, rx, by, ty,
      0, 1, ymin, ymax,
      xticks, [0.0, 0.5, 1.0],
      pi === 3 ? 'Ray distance  t' : '',
      'Weight', '');

    // unsampled region: from 0 to t_1
    const t1 = Math.pow(1 / N, a);
    ctx.save();
    ctx.fillStyle = 'rgba(255,180,180,0.35)';
    ctx.fillRect(xs(0), ty, xs(t1) - xs(0), by - ty);
    ctx.restore();

    // label "unsampled"
    ctx.save();
    ctx.fillStyle = '#c44';
    ctx.font = 'italic 10px sans-serif';
    ctx.textAlign = 'left';
    ctx.fillText('unsampled', xs(0.01), (ty + by) / 2 - 4);
    ctx.restore();

    // bars
    for (let i = 1; i <= N; i++) {
      const t = Math.pow(i / N, a);
      const w = 1 / i;
      const bw = (rx - lx) * 0.018;
      const bx = xs(t) - bw / 2;
      const bh = (by - ty) * w / (ymax - ymin);

      ctx.save();
      ctx.fillStyle = cols[pi];
      ctx.fillRect(bx, by - bh, bw, bh);
      ctx.restore();

      // weight label
      const label = i === 1 ? '1' : `1/${i}`;
      ctx.save();
      ctx.fillStyle = '#000';
      ctx.font = '10px sans-serif';
      ctx.textAlign = 'center';
      ctx.fillText(label, xs(t), by - bh - 4);
      ctx.restore();
    }

    // alpha label (top right)
    ctx.save();
    ctx.fillStyle = cols[pi];
    ctx.font = 'bold 11px sans-serif';
    ctx.textAlign = 'right';
    ctx.fillText(names[pi], rx - 4, ty + 14);
    ctx.restore();
  });
})();
</script>
</body>
</html>
